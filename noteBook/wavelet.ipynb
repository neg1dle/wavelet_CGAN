{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da51e13-2605-4abc-a8a4-8094794da254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\sit\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\sit\\anaconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: pywavelets in c:\\users\\sit\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sit\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\sit\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sit\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sit\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sit\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sit\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sit\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sit\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sit\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sit\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sit\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sit\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sit\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision pywavelets tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91584616-67d6-470e-a185-a7dd9aa3f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 625/625 [00:33<00:00, 18.50it/s, D_loss=0.163, G_loss=9.36] \n",
      "Epoch 2/100: 100%|██████████| 625/625 [00:34<00:00, 18.31it/s, D_loss=0.0113, G_loss=11.4] \n",
      "Epoch 3/100: 100%|██████████| 625/625 [00:35<00:00, 17.72it/s, D_loss=0.0121, G_loss=12.4] \n",
      "Epoch 4/100: 100%|██████████| 625/625 [00:35<00:00, 17.62it/s, D_loss=0.0122, G_loss=10.4] \n",
      "Epoch 5/100: 100%|██████████| 625/625 [00:35<00:00, 17.85it/s, D_loss=0.00796, G_loss=11.4]\n",
      "Epoch 6/100: 100%|██████████| 625/625 [00:34<00:00, 17.93it/s, D_loss=0.531, G_loss=6.19]  \n",
      "Epoch 7/100: 100%|██████████| 625/625 [00:35<00:00, 17.67it/s, D_loss=0.773, G_loss=5.09] \n",
      "Epoch 8/100: 100%|██████████| 625/625 [00:34<00:00, 18.34it/s, D_loss=0.234, G_loss=7.48] \n",
      "Epoch 9/100: 100%|██████████| 625/625 [00:37<00:00, 16.68it/s, D_loss=0.521, G_loss=5.48] \n",
      "Epoch 10/100: 100%|██████████| 625/625 [00:38<00:00, 16.23it/s, D_loss=0.368, G_loss=6.92]\n",
      "Epoch 11/100: 100%|██████████| 625/625 [00:37<00:00, 16.65it/s, D_loss=0.404, G_loss=8.54] \n",
      "Epoch 12/100: 100%|██████████| 625/625 [00:36<00:00, 17.31it/s, D_loss=0.435, G_loss=6.83] \n",
      "Epoch 13/100: 100%|██████████| 625/625 [00:40<00:00, 15.58it/s, D_loss=0.696, G_loss=4.65]\n",
      "Epoch 14/100: 100%|██████████| 625/625 [00:38<00:00, 16.12it/s, D_loss=0.43, G_loss=6.47] \n",
      "Epoch 15/100: 100%|██████████| 625/625 [00:37<00:00, 16.88it/s, D_loss=0.511, G_loss=6.04]\n",
      "Epoch 16/100: 100%|██████████| 625/625 [00:37<00:00, 16.71it/s, D_loss=0.609, G_loss=6.32]\n",
      "Epoch 17/100: 100%|██████████| 625/625 [00:36<00:00, 17.18it/s, D_loss=0.463, G_loss=6.13]\n",
      "Epoch 18/100: 100%|██████████| 625/625 [00:37<00:00, 16.54it/s, D_loss=0.433, G_loss=6.51]\n",
      "Epoch 19/100: 100%|██████████| 625/625 [00:38<00:00, 16.10it/s, D_loss=0.748, G_loss=5.15]\n",
      "Epoch 20/100: 100%|██████████| 625/625 [00:37<00:00, 16.60it/s, D_loss=0.651, G_loss=4.57]\n",
      "Epoch 21/100: 100%|██████████| 625/625 [00:37<00:00, 16.70it/s, D_loss=0.496, G_loss=5.91]\n",
      "Epoch 22/100: 100%|██████████| 625/625 [00:37<00:00, 16.51it/s, D_loss=0.546, G_loss=6.46]\n",
      "Epoch 23/100: 100%|██████████| 625/625 [00:38<00:00, 16.16it/s, D_loss=0.608, G_loss=6.03]\n",
      "Epoch 24/100: 100%|██████████| 625/625 [00:39<00:00, 15.84it/s, D_loss=0.604, G_loss=5.78]\n",
      "Epoch 25/100: 100%|██████████| 625/625 [00:38<00:00, 16.34it/s, D_loss=0.606, G_loss=4.58]\n",
      "Epoch 26/100: 100%|██████████| 625/625 [00:38<00:00, 16.12it/s, D_loss=0.509, G_loss=5.63]\n",
      "Epoch 27/100: 100%|██████████| 625/625 [00:38<00:00, 16.38it/s, D_loss=0.616, G_loss=4.69]\n",
      "Epoch 28/100: 100%|██████████| 625/625 [00:37<00:00, 16.61it/s, D_loss=0.668, G_loss=4.55]\n",
      "Epoch 29/100: 100%|██████████| 625/625 [00:37<00:00, 16.80it/s, D_loss=0.494, G_loss=5.81]\n",
      "Epoch 30/100: 100%|██████████| 625/625 [00:37<00:00, 16.82it/s, D_loss=0.59, G_loss=5.63] \n",
      "Epoch 31/100: 100%|██████████| 625/625 [00:36<00:00, 17.14it/s, D_loss=0.551, G_loss=6.21]\n",
      "Epoch 32/100: 100%|██████████| 625/625 [00:36<00:00, 17.10it/s, D_loss=0.676, G_loss=5.43]\n",
      "Epoch 33/100: 100%|██████████| 625/625 [00:37<00:00, 16.75it/s, D_loss=0.623, G_loss=4.86]\n",
      "Epoch 34/100: 100%|██████████| 625/625 [00:37<00:00, 16.47it/s, D_loss=0.797, G_loss=3.9] \n",
      "Epoch 35/100: 100%|██████████| 625/625 [00:38<00:00, 16.39it/s, D_loss=0.517, G_loss=5.21]\n",
      "Epoch 36/100: 100%|██████████| 625/625 [00:38<00:00, 16.40it/s, D_loss=0.59, G_loss=5.51] \n",
      "Epoch 37/100: 100%|██████████| 625/625 [00:40<00:00, 15.57it/s, D_loss=0.594, G_loss=3.97]\n",
      "Epoch 38/100: 100%|██████████| 625/625 [00:40<00:00, 15.26it/s, D_loss=0.559, G_loss=5.47]\n",
      "Epoch 39/100: 100%|██████████| 625/625 [00:38<00:00, 16.17it/s, D_loss=6.28, G_loss=5.59] \n",
      "Epoch 40/100: 100%|██████████| 625/625 [00:36<00:00, 17.22it/s, D_loss=0.536, G_loss=5.3] \n",
      "Epoch 41/100: 100%|██████████| 625/625 [00:36<00:00, 17.24it/s, D_loss=0.551, G_loss=6.34]\n",
      "Epoch 42/100: 100%|██████████| 625/625 [00:36<00:00, 17.25it/s, D_loss=0.632, G_loss=5.33]\n",
      "Epoch 43/100: 100%|██████████| 625/625 [00:35<00:00, 17.58it/s, D_loss=0.653, G_loss=5.03]\n",
      "Epoch 44/100: 100%|██████████| 625/625 [00:36<00:00, 17.12it/s, D_loss=0.508, G_loss=5.36]\n",
      "Epoch 45/100: 100%|██████████| 625/625 [00:37<00:00, 16.86it/s, D_loss=0.603, G_loss=5.92]\n",
      "Epoch 46/100: 100%|██████████| 625/625 [00:36<00:00, 17.01it/s, D_loss=0.743, G_loss=4.87]\n",
      "Epoch 47/100: 100%|██████████| 625/625 [00:38<00:00, 16.42it/s, D_loss=0.61, G_loss=5.95] \n",
      "Epoch 48/100: 100%|██████████| 625/625 [00:38<00:00, 16.44it/s, D_loss=0.578, G_loss=5.33]\n",
      "Epoch 49/100: 100%|██████████| 625/625 [00:38<00:00, 16.23it/s, D_loss=0.66, G_loss=6.5]  \n",
      "Epoch 50/100: 100%|██████████| 625/625 [00:37<00:00, 16.83it/s, D_loss=0.59, G_loss=5.5]  \n",
      "Epoch 51/100: 100%|██████████| 625/625 [00:37<00:00, 16.80it/s, D_loss=0.585, G_loss=4.83]\n",
      "Epoch 52/100: 100%|██████████| 625/625 [00:38<00:00, 16.28it/s, D_loss=0.646, G_loss=5.53]\n",
      "Epoch 53/100: 100%|██████████| 625/625 [00:36<00:00, 17.22it/s, D_loss=0.847, G_loss=4.97]\n",
      "Epoch 54/100: 100%|██████████| 625/625 [00:35<00:00, 17.65it/s, D_loss=0.527, G_loss=5]   \n",
      "Epoch 55/100: 100%|██████████| 625/625 [00:35<00:00, 17.75it/s, D_loss=0.696, G_loss=4.74]\n",
      "Epoch 56/100: 100%|██████████| 625/625 [00:35<00:00, 17.47it/s, D_loss=0.593, G_loss=5.79]\n",
      "Epoch 57/100: 100%|██████████| 625/625 [00:37<00:00, 16.70it/s, D_loss=0.626, G_loss=5.13]\n",
      "Epoch 58/100: 100%|██████████| 625/625 [00:36<00:00, 16.93it/s, D_loss=0.534, G_loss=6.62]\n",
      "Epoch 59/100: 100%|██████████| 625/625 [00:37<00:00, 16.45it/s, D_loss=0.532, G_loss=5.24]\n",
      "Epoch 60/100: 100%|██████████| 625/625 [00:38<00:00, 16.13it/s, D_loss=0.791, G_loss=5.54]\n",
      "Epoch 61/100: 100%|██████████| 625/625 [00:39<00:00, 15.97it/s, D_loss=0.578, G_loss=6.37]\n",
      "Epoch 62/100: 100%|██████████| 625/625 [00:38<00:00, 16.21it/s, D_loss=0.796, G_loss=5.38]\n",
      "Epoch 63/100: 100%|██████████| 625/625 [00:39<00:00, 15.93it/s, D_loss=0.613, G_loss=4.03]\n",
      "Epoch 64/100: 100%|██████████| 625/625 [00:38<00:00, 16.06it/s, D_loss=0.594, G_loss=5.48]\n",
      "Epoch 65/100: 100%|██████████| 625/625 [00:38<00:00, 16.42it/s, D_loss=0.635, G_loss=5.7] \n",
      "Epoch 66/100: 100%|██████████| 625/625 [00:37<00:00, 16.51it/s, D_loss=0.643, G_loss=6.63]\n",
      "Epoch 67/100: 100%|██████████| 625/625 [00:36<00:00, 16.92it/s, D_loss=0.571, G_loss=4.73]\n",
      "Epoch 68/100: 100%|██████████| 625/625 [00:37<00:00, 16.77it/s, D_loss=0.684, G_loss=4.41]\n",
      "Epoch 69/100: 100%|██████████| 625/625 [00:36<00:00, 17.09it/s, D_loss=0.61, G_loss=5.81] \n",
      "Epoch 70/100: 100%|██████████| 625/625 [00:36<00:00, 16.93it/s, D_loss=0.546, G_loss=6.73]\n",
      "Epoch 71/100: 100%|██████████| 625/625 [00:36<00:00, 17.06it/s, D_loss=0.727, G_loss=4.8] \n",
      "Epoch 72/100: 100%|██████████| 625/625 [00:36<00:00, 16.98it/s, D_loss=0.505, G_loss=4.93]\n",
      "Epoch 73/100: 100%|██████████| 625/625 [00:38<00:00, 16.24it/s, D_loss=0.568, G_loss=5.48]\n",
      "Epoch 74/100: 100%|██████████| 625/625 [00:38<00:00, 16.11it/s, D_loss=0.595, G_loss=5.05]\n",
      "Epoch 75/100: 100%|██████████| 625/625 [00:39<00:00, 15.83it/s, D_loss=0.914, G_loss=5.59]\n",
      "Epoch 76/100: 100%|██████████| 625/625 [00:39<00:00, 15.68it/s, D_loss=0.795, G_loss=5]   \n",
      "Epoch 77/100: 100%|██████████| 625/625 [00:38<00:00, 16.09it/s, D_loss=0.793, G_loss=5.38]\n",
      "Epoch 78/100: 100%|██████████| 625/625 [00:37<00:00, 16.78it/s, D_loss=0.565, G_loss=4.68]\n",
      "Epoch 79/100: 100%|██████████| 625/625 [00:37<00:00, 16.86it/s, D_loss=0.584, G_loss=5.48]\n",
      "Epoch 80/100: 100%|██████████| 625/625 [00:36<00:00, 17.05it/s, D_loss=0.66, G_loss=5.42] \n",
      "Epoch 81/100: 100%|██████████| 625/625 [00:36<00:00, 17.03it/s, D_loss=0.633, G_loss=4.61]\n",
      "Epoch 82/100: 100%|██████████| 625/625 [00:36<00:00, 17.17it/s, D_loss=0.657, G_loss=4.59]\n",
      "Epoch 83/100: 100%|██████████| 625/625 [00:37<00:00, 16.61it/s, D_loss=0.489, G_loss=6.45]\n",
      "Epoch 84/100: 100%|██████████| 625/625 [00:38<00:00, 16.21it/s, D_loss=0.585, G_loss=5.2] \n",
      "Epoch 85/100: 100%|██████████| 625/625 [00:40<00:00, 15.61it/s, D_loss=0.616, G_loss=4.78]\n",
      "Epoch 86/100: 100%|██████████| 625/625 [00:41<00:00, 15.23it/s, D_loss=0.606, G_loss=4.98]\n",
      "Epoch 87/100: 100%|██████████| 625/625 [00:37<00:00, 16.48it/s, D_loss=0.612, G_loss=4.4] \n",
      "Epoch 88/100: 100%|██████████| 625/625 [00:38<00:00, 16.03it/s, D_loss=0.555, G_loss=5.23]\n",
      "Epoch 89/100: 100%|██████████| 625/625 [00:37<00:00, 16.76it/s, D_loss=0.55, G_loss=6.35] \n",
      "Epoch 90/100: 100%|██████████| 625/625 [00:36<00:00, 17.15it/s, D_loss=0.726, G_loss=4.97]\n",
      "Epoch 91/100: 100%|██████████| 625/625 [00:36<00:00, 16.99it/s, D_loss=0.559, G_loss=4.97]\n",
      "Epoch 92/100: 100%|██████████| 625/625 [00:35<00:00, 17.43it/s, D_loss=0.773, G_loss=5.84]\n",
      "Epoch 93/100: 100%|██████████| 625/625 [00:35<00:00, 17.44it/s, D_loss=0.615, G_loss=6.4] \n",
      "Epoch 94/100: 100%|██████████| 625/625 [00:37<00:00, 16.84it/s, D_loss=0.536, G_loss=5.17]\n",
      "Epoch 95/100: 100%|██████████| 625/625 [00:38<00:00, 16.31it/s, D_loss=0.712, G_loss=4.37]\n",
      "Epoch 96/100: 100%|██████████| 625/625 [00:38<00:00, 16.13it/s, D_loss=0.553, G_loss=5.52]\n",
      "Epoch 97/100: 100%|██████████| 625/625 [00:38<00:00, 16.05it/s, D_loss=0.579, G_loss=6.23]\n",
      "Epoch 98/100: 100%|██████████| 625/625 [00:38<00:00, 16.12it/s, D_loss=0.532, G_loss=5.41]\n",
      "Epoch 99/100: 100%|██████████| 625/625 [00:40<00:00, 15.56it/s, D_loss=0.65, G_loss=5.01] \n",
      "Epoch 100/100: 100%|██████████| 625/625 [00:39<00:00, 15.84it/s, D_loss=0.591, G_loss=5]   \n"
     ]
    }
   ],
   "source": [
    "#GANの学習\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ========== Dataset ==========\n",
    "class AE2CleanDataset(Dataset):\n",
    "    def __init__(self, ae_dir, clean_dir, transform=None):\n",
    "        self.ae_dir = ae_dir\n",
    "        self.clean_dir = clean_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = sorted(os.listdir(ae_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ae_path = os.path.join(self.ae_dir, self.filenames[idx])\n",
    "        clean_path = os.path.join(self.clean_dir, self.filenames[idx])\n",
    "\n",
    "        ae_img = Image.open(ae_path).convert(\"RGB\")\n",
    "        clean_img = Image.open(clean_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            ae_img = self.transform(ae_img)\n",
    "            clean_img = self.transform(clean_img)\n",
    "\n",
    "        return ae_img, clean_img\n",
    "\n",
    "\n",
    "# ========== Generator (U-Net) ==========\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            self.block(features * 4, features * 8),\n",
    "            self.block(features * 8, features * 8),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upblock(features * 8, features * 8),\n",
    "            self.upblock(features * 8 * 2, features * 4),\n",
    "            self.upblock(features * 4 * 2, features * 2),\n",
    "            self.upblock(features * 2 * 2, features),\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upblock(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        skips = skips[:-1][::-1]  # reverse except last\n",
    "        for idx, layer in enumerate(self.decoder[:-2]):\n",
    "            x = layer(x)\n",
    "            if idx < len(skips):\n",
    "                x = torch.cat([x, skips[idx]], 1)\n",
    "\n",
    "        return self.decoder[-2](x)\n",
    "\n",
    "\n",
    "# ========== Discriminator ==========\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=6, features=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            nn.Conv2d(features * 4, 1, kernel_size=4, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.net(torch.cat([x, y], 1))\n",
    "\n",
    "\n",
    "# ========== 学習ループ ==========\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    transform = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "\n",
    "    dataset = AE2CleanDataset(\"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\train\\\\AE_wavelet\",\"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\train\\\\normal256\" , transform)\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    gen = UNetGenerator().to(device)\n",
    "    disc = PatchDiscriminator().to(device)\n",
    "\n",
    "    opt_g = torch.optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    opt_d = torch.optim.Adam(disc.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "    bce = nn.BCELoss()\n",
    "    l1 = nn.L1Loss()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch+1}/100\")\n",
    "        for ae_img, clean_img in loop:\n",
    "            ae_img, clean_img = ae_img.to(device), clean_img.to(device)\n",
    "\n",
    "            # === Discriminator ===\n",
    "            fake_img = gen(ae_img).detach()\n",
    "            real_pred = disc(ae_img, clean_img)\n",
    "            fake_pred = disc(ae_img, fake_img)\n",
    "\n",
    "            real_loss = bce(real_pred, torch.ones_like(real_pred))\n",
    "            fake_loss = bce(fake_pred, torch.zeros_like(fake_pred))\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            opt_d.zero_grad()\n",
    "            d_loss.backward()\n",
    "            opt_d.step()\n",
    "\n",
    "            # === Generator ===\n",
    "            fake_img = gen(ae_img)\n",
    "            pred = disc(ae_img, fake_img)\n",
    "            adv_loss = bce(pred, torch.ones_like(pred))\n",
    "            l1_loss = l1(fake_img, clean_img) * 100\n",
    "\n",
    "            g_loss = adv_loss + l1_loss\n",
    "\n",
    "            opt_g.zero_grad()\n",
    "            g_loss.backward()\n",
    "            opt_g.step()\n",
    "\n",
    "            loop.set_postfix(G_loss=g_loss.item(), D_loss=d_loss.item())\n",
    "\n",
    "        # Save checkpoints\n",
    "        torch.save(gen.state_dict(), f\"./gen_weights_epoch{epoch+1}.pth\")\n",
    "        save_image(fake_img * 0.5 + 0.5, f\"./outputs/fake_epoch{epoch+1}.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c88aa2ea-ce2c-4604-ba61-e8ead3ab64dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wavelet変換中: 0it [00:00, ?it/s]\n",
      "Epoch [1/100]:  54%|█████▎    | 669/1250 [01:20<01:10,  8.26it/s, loss_D=0.0136, loss_G=10.2] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 352\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    351\u001b[0m     save_wavelet_clean_images(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/AE2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/CLEAN2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 352\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 317\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    316\u001b[0m     loop \u001b[38;5;241m=\u001b[39m tqdm(dataloader)\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (ae_imgs, clean_imgs) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loop):\n\u001b[0;32m    318\u001b[0m         ae_imgs \u001b[38;5;241m=\u001b[39m ae_imgs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    319\u001b[0m         clean_imgs \u001b[38;5;241m=\u001b[39m clean_imgs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch260\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch260\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch260\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch260\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch260\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m, in \u001b[0;36mAEDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     35\u001b[0m clean_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_dir, name)\n\u001b[0;32m     36\u001b[0m ae_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(ae_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m clean_img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     39\u001b[0m     ae_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(ae_img)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch260\\lib\\site-packages\\PIL\\Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch260\\lib\\site-packages\\PIL\\ImageFile.py:280\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 280\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecodermaxblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch260\\lib\\site-packages\\PIL\\JpegImagePlugin.py:404\u001b[0m, in \u001b[0;36mJpegImageFile.load_read\u001b[1;34m(self, read_bytes)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, read_bytes: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    internal: read more image data\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m    so libjpeg can finish decoding\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 404\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ended\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;66;03m# Premature EOF.\u001b[39;00m\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;66;03m# Pretend file is finished adding EOI marker\u001b[39;00m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#AE画像からwavelet変換し、cleanに保存\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pywt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 作業ディレクトリに変更\n",
    "os.chdir(\"C:\\\\Users\\\\sit\\\\wavelet_CGAN\")\n",
    "\n",
    "# ======== Dataset ========\n",
    "class AEDataset(Dataset):\n",
    "    def __init__(self, ae_dir, clean_dir, transform=None):\n",
    "        self.ae_dir = ae_dir\n",
    "        self.clean_dir = clean_dir\n",
    "        self.names = [f for f in os.listdir(ae_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names[idx]\n",
    "        ae_path = os.path.join(self.ae_dir, name)\n",
    "        clean_path = os.path.join(self.clean_dir, name)\n",
    "        ae_img = Image.open(ae_path).convert('RGB')\n",
    "        clean_img = Image.open(clean_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            ae_img = self.transform(ae_img)\n",
    "            clean_img = self.transform(clean_img)\n",
    "        return ae_img, clean_img\n",
    "\n",
    "# ======== Waveletノイズ除去 ========\n",
    "def wavelet_denoise(img_tensor, wavelet='haar', level=1, threshold=0.00001):\n",
    "    with torch.no_grad():\n",
    "        img_np = img_tensor.cpu().numpy()\n",
    "        denoised = []\n",
    "        for b in range(img_np.shape[0]):\n",
    "            channels = []\n",
    "            for c in range(img_np.shape[1]):\n",
    "                coeffs = pywt.wavedec2(img_np[b, c], wavelet=wavelet, level=level)\n",
    "                cA, cD = coeffs[0], coeffs[1:]\n",
    "                cD_thresh = []\n",
    "                for (cH, cV, cD_) in cD:\n",
    "                    cH = pywt.threshold(cH, threshold * np.max(cH))\n",
    "                    cV = pywt.threshold(cV, threshold * np.max(cV))\n",
    "                    cD_ = pywt.threshold(cD_, threshold * np.max(cD_))\n",
    "                    cD_thresh.append((cH, cV, cD_))\n",
    "                coeffs_thresh = [cA] + cD_thresh\n",
    "                denoised_channel = pywt.waverec2(coeffs_thresh, wavelet)\n",
    "                denoised_channel = denoised_channel[:img_np.shape[2], :img_np.shape[3]]\n",
    "                channels.append(denoised_channel)\n",
    "            denoised.append(np.stack(channels))\n",
    "        denoised = np.stack(denoised)\n",
    "        return torch.tensor(denoised).to(img_tensor.device).float().clamp(-1, 1)\n",
    "\n",
    "def save_wavelet_clean_images(ae_dir, clean_dir):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    os.makedirs(clean_dir, exist_ok=True)\n",
    "    files = [f for f in os.listdir(ae_dir) if f.endswith(('.png', '.jpg'))]\n",
    "    for name in tqdm(files, desc=\"Wavelet保存\"):\n",
    "        img = Image.open(os.path.join(ae_dir, name)).convert('RGB')\n",
    "        img_tensor = transform(img).unsqueeze(0)\n",
    "        denoised = wavelet_denoise(img_tensor)\n",
    "        out = denoised.squeeze(0).cpu() * 0.5 + 0.5\n",
    "        Image.fromarray((out.permute(1,2,0).numpy() * 255).astype(np.uint8)).save(os.path.join(clean_dir, name))\n",
    "\n",
    "# ======== Wavelet Layer ========\n",
    "class DWTForward(nn.Module):\n",
    "    def __init__(self, wave='haar'):\n",
    "        super().__init__()\n",
    "        self.wave = wave\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        coeffs = []\n",
    "        for b in range(B):\n",
    "            c_stack = []\n",
    "            for c in range(C):\n",
    "                cA, (cH, cV, cD) = pywt.dwt2(x[b, c].cpu().numpy(), self.wave)\n",
    "                c_stack.append(np.stack([cA, cH, cV, cD]))\n",
    "            coeffs.append(np.stack(c_stack))\n",
    "        coeffs = torch.tensor(coeffs).to(x.device)\n",
    "        return coeffs\n",
    "\n",
    "class DWTInverse(nn.Module):\n",
    "    def __init__(self, wave='haar'):\n",
    "        super().__init__()\n",
    "        self.wave = wave\n",
    "\n",
    "    def forward(self, coeffs):\n",
    "        B, C, _, H, W = coeffs.shape\n",
    "        imgs = []\n",
    "        for b in range(B):\n",
    "            rec = []\n",
    "            for c in range(C):\n",
    "                cA, cH, cV, cD = coeffs[b, c]\n",
    "                i = pywt.idwt2((cA.cpu().numpy(), (cH.cpu().numpy(), cV.cpu().numpy(), cD.cpu().numpy())), self.wave)\n",
    "                rec.append(i)\n",
    "            imgs.append(np.stack(rec))\n",
    "        return torch.tensor(imgs).to(coeffs.device)\n",
    "\n",
    "# ======== WaveCNet Generator ========\n",
    "class WaveCNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.dwt = DWTForward()\n",
    "        self.iwt = DWTInverse()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 4, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, out_channels * 4, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        coeffs = self.dwt(x)\n",
    "        B, C, W, H, W = coeffs.size()\n",
    "        coeffs = coeffs.view(B, C * 4, H, W)\n",
    "        enc = self.encoder(coeffs)\n",
    "        dec = self.decoder(enc)\n",
    "        dec = dec.view(B, C, 4, H, W)\n",
    "        return self.iwt(dec)\n",
    "\n",
    "# ======== PatchGAN Discriminator ========\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=6):\n",
    "        super().__init__()\n",
    "        def block(in_f, out_f, norm=True):\n",
    "            layers = [nn.Conv2d(in_f, out_f, 4, 2, 1)]\n",
    "            if norm:\n",
    "                layers.append(nn.BatchNorm2d(out_f))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            return layers\n",
    "        self.model = nn.Sequential(\n",
    "            *block(in_channels, 64, norm=False),\n",
    "            *block(64, 128),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        return self.model(torch.cat((img_A, img_B), 1))\n",
    "\n",
    "\n",
    "# ======== AE → wavelet変換画像を保存 =========\n",
    "def save_wavelet_clean_images(ae_dir, clean_dir, wavelet='haar', level=1, threshold=0.00001):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    if not os.path.exists(clean_dir):\n",
    "        os.makedirs(clean_dir)\n",
    "\n",
    "    file_names = [f for f in os.listdir(ae_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    for name in tqdm(file_names, desc=\"Wavelet変換中\"):\n",
    "        img_path = os.path.join(ae_dir, name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "        denoised_tensor = wavelet_denoise(img_tensor, wavelet=wavelet, level=level, threshold=threshold)\n",
    "        denoised_img = denoised_tensor.squeeze(0).cpu()\n",
    "        denoised_img = denoised_img * 0.5 + 0.5  # [-1,1] → [0,1]\n",
    "        denoised_img = transforms.ToPILImage()(denoised_img)\n",
    "        denoised_img.save(os.path.join(clean_dir, name))\n",
    "\n",
    "# ======== Generator (U-Net) ========\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_size))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.dropout2d(x, p=self.dropout) if self.dropout else x\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        if self.dropout:\n",
    "            x = F.dropout2d(x, p=self.dropout)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "        return x\n",
    "\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
    "\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up5 = UNetUp(1024, 256)\n",
    "        self.up6 = UNetUp(512, 128)\n",
    "        self.up7 = UNetUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "        return self.final(u7)\n",
    "\n",
    "# ======== Discriminator (PatchGAN) ========\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=6):\n",
    "        super().__init__()\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, 2, 1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        x = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(x)\n",
    "\n",
    "# ======== 損失関数 ========\n",
    "criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "\n",
    "# ======== 学習 ========\n",
    "def train():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    dataset = AEDataset('train/AE', 'train/CLEAN')\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "    generator = GeneratorUNet().to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "    real_label = 1.\n",
    "    fake_label = 0.\n",
    "\n",
    "    epochs = 100\n",
    "    lambda_L1 = 100\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(dataloader)\n",
    "        for i, (ae_imgs, clean_imgs) in enumerate(loop):\n",
    "            ae_imgs = ae_imgs.to(device)\n",
    "            clean_imgs = clean_imgs.to(device)\n",
    "\n",
    "            ae_denoised = wavelet_denoise(ae_imgs)\n",
    "\n",
    "            # Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            fake_clean = generator(ae_denoised)\n",
    "            pred_real = discriminator(ae_denoised, clean_imgs)\n",
    "            pred_fake = discriminator(ae_denoised, fake_clean.detach())\n",
    "\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real) * real_label)\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.ones_like(pred_fake) * fake_label)\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            pred_fake = discriminator(ae_denoised, fake_clean)\n",
    "            loss_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake) * real_label)\n",
    "            loss_L1_value = criterion_L1(fake_clean, clean_imgs) * lambda_L1\n",
    "            loss_G = loss_GAN + loss_L1_value\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "            loop.set_postfix(loss_D=loss_D.item(), loss_G=loss_G.item())\n",
    "\n",
    "    torch.save(generator.state_dict(), 'generator.pth')\n",
    "    torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_wavelet_clean_images('train/AE2', 'train/CLEAN2')\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dce2c4a-aa2d-4c85-a458-782bbb0e13a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:12<00:00, 69.40it/s]\n",
      "100%|██████████| 5000/5000 [01:07<00:00, 74.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveletノイズ除去画像を C:\\Users\\sit\\wavelet_CGAN\\train\\AE_wavelet に保存しました。\n",
      "復元画像を C:\\Users\\sit\\wavelet_CGAN\\train\\AE_wavelet_GAN に保存しました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pywt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== Waveletノイズ除去関数 ==========\n",
    "def wavelet_denoise(img_tensor, wavelet='haar', level=1, threshold=0.05):\n",
    "    with torch.no_grad():\n",
    "        img_np = img_tensor.cpu().numpy()\n",
    "        denoised = []\n",
    "        for b in range(img_np.shape[0]):\n",
    "            channels = []\n",
    "            for c in range(img_np.shape[1]):\n",
    "                coeffs = pywt.wavedec2(img_np[b, c], wavelet=wavelet, level=level)\n",
    "                cA, cD = coeffs[0], coeffs[1:]\n",
    "                cD_thresh = []\n",
    "                for (cH, cV, cD_) in cD:\n",
    "                    cH = pywt.threshold(cH, threshold * np.max(np.abs(cH)))\n",
    "                    cV = pywt.threshold(cV, threshold * np.max(np.abs(cV)))\n",
    "                    cD_ = pywt.threshold(cD_, threshold * np.max(np.abs(cD_)))\n",
    "                    cD_thresh.append((cH, cV, cD_))\n",
    "                coeffs_thresh = [cA] + cD_thresh\n",
    "                denoised_channel = pywt.waverec2(coeffs_thresh, wavelet)\n",
    "                # 元のサイズに切り詰め\n",
    "                denoised_channel = denoised_channel[:img_np.shape[2], :img_np.shape[3]]\n",
    "                channels.append(denoised_channel)\n",
    "            denoised.append(np.stack(channels))\n",
    "        denoised = np.stack(denoised)\n",
    "        return torch.tensor(denoised).to(img_tensor.device).float().clamp(-1, 1)\n",
    "\n",
    "# ========== GeneratorUNetモデル ==========\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_size))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        if self.dropout:\n",
    "            x = F.dropout2d(x, p=self.dropout)\n",
    "        return x\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        if self.dropout:\n",
    "            x = F.dropout2d(x, p=self.dropout)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "        return x\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            self.block(features * 4, features * 8),\n",
    "            self.block(features * 8, features * 8),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upblock(features * 8, features * 8),\n",
    "            self.upblock(features * 8 * 2, features * 4),\n",
    "            self.upblock(features * 4 * 2, features * 2),\n",
    "            self.upblock(features * 2 * 2, features),\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upblock(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        skips = skips[:-1][::-1]  # reverse except last\n",
    "        for idx, layer in enumerate(self.decoder[:-2]):\n",
    "            x = layer(x)\n",
    "            if idx < len(skips):\n",
    "                x = torch.cat([x, skips[idx]], 1)\n",
    "\n",
    "        return self.decoder[-2](x)\n",
    "\n",
    "\n",
    "# ========== 推論処理 ==========\n",
    "def load_generator(model_path, device):\n",
    "    model = UNetGenerator().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def denoise_and_correct(generator, input_path, output_path, device):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    img = Image.open(input_path).convert('RGB')\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)  # (1,3,H,W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_denoised = wavelet_denoise(input_tensor)\n",
    "        output = generator(input_denoised)\n",
    "        output_img = (output.squeeze(0).cpu() * 0.5) + 0.5  # [-1,1] -> [0,1]\n",
    "\n",
    "    output_pil = transforms.ToPILImage()(output_img.clamp(0,1))\n",
    "    output_pil.save(output_path)\n",
    "\n",
    "\n",
    "def wavelet_denoise_and_save(input_path, output_path, device):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    img = Image.open(input_path).convert('RGB')\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)  # (1,3,H,W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        denoised_tensor = wavelet_denoise(input_tensor)\n",
    "\n",
    "    # denoised_tensorは[-1,1]範囲なので[0,1]に変換\n",
    "    denoised_img = (denoised_tensor.squeeze(0).cpu() * 0.5) + 0.5\n",
    "    denoised_pil = transforms.ToPILImage()(denoised_img.clamp(0,1))\n",
    "    denoised_pil.save(output_path)\n",
    "\n",
    "    \n",
    "\n",
    "# ========== メイン処理 ==========\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator = load_generator(\"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\gen_weights_epoch100.pth\", device)\n",
    "\n",
    "    input_dir = \"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\train\\\\AE\"\n",
    "    wavelet_dir = \"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\train\\\\AE_wavelet\"\n",
    "    output_dir = \"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\train\\\\AE_wavelet_GAN\"\n",
    "    \n",
    "    os.makedirs(wavelet_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Waveletノイズ除去画像を別フォルダに保存\n",
    "    for fname in tqdm(os.listdir(input_dir)):\n",
    "        if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            continue\n",
    "        input_path = os.path.join(input_dir, fname)\n",
    "        wavelet_path = os.path.join(wavelet_dir, fname)\n",
    "        wavelet_denoise_and_save(input_path, wavelet_path, device)\n",
    "\n",
    "    # 2. GANで復元処理（Waveletノイズ除去画像を入力に）\n",
    "    for fname in tqdm(os.listdir(wavelet_dir)):\n",
    "        if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            continue\n",
    "        input_path = os.path.join(wavelet_dir, fname)\n",
    "        output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "        img = Image.open(input_path).convert('RGB')\n",
    "        input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = generator(input_tensor)\n",
    "            output_img = (output.squeeze(0).cpu() * 0.5) + 0.5  # [-1,1]→[0,1]\n",
    "\n",
    "        output_pil = transforms.ToPILImage()(output_img.clamp(0,1))\n",
    "        output_pil.save(output_path)\n",
    "\n",
    "    print(f\"Waveletノイズ除去画像を {wavelet_dir} に保存しました。\")\n",
    "    print(f\"復元画像を {output_dir} に保存しました。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e46349ea-9b84-4c4d-818d-eaaf2e636a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [00:06<00:00, 145.87it/s]\n",
      "100%|██████████| 998/998 [00:04<00:00, 204.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveletノイズ除去画像を C:\\Users\\sit\\wavelet_CGAN\\testdata2\\clean に保存しました。\n",
      "復元画像を C:\\Users\\sit\\wavelet_CGAN\\testdata2\\corrected に保存しました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pywt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== Waveletノイズ除去関数 ==========\n",
    "def wavelet_denoise(img_tensor, wavelet='haar', level=1, threshold=0.00001):\n",
    "    with torch.no_grad():\n",
    "        img_np = img_tensor.cpu().numpy()\n",
    "        denoised = []\n",
    "        for b in range(img_np.shape[0]):\n",
    "            channels = []\n",
    "            for c in range(img_np.shape[1]):\n",
    "                coeffs = pywt.wavedec2(img_np[b, c], wavelet=wavelet, level=level)\n",
    "                cA, cD = coeffs[0], coeffs[1:]\n",
    "                cD_thresh = []\n",
    "                for (cH, cV, cD_) in cD:\n",
    "                    cH = pywt.threshold(cH, threshold * np.max(np.abs(cH)))\n",
    "                    cV = pywt.threshold(cV, threshold * np.max(np.abs(cV)))\n",
    "                    cD_ = pywt.threshold(cD_, threshold * np.max(np.abs(cD_)))\n",
    "                    cD_thresh.append((cH, cV, cD_))\n",
    "                coeffs_thresh = [cA] + cD_thresh\n",
    "                denoised_channel = pywt.waverec2(coeffs_thresh, wavelet)\n",
    "                # 元のサイズに切り詰め\n",
    "                denoised_channel = denoised_channel[:img_np.shape[2], :img_np.shape[3]]\n",
    "                channels.append(denoised_channel)\n",
    "            denoised.append(np.stack(channels))\n",
    "        denoised = np.stack(denoised)\n",
    "        return torch.tensor(denoised).to(img_tensor.device).float().clamp(-1, 1)\n",
    "\n",
    "# ========== GeneratorUNetモデル ==========\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_size))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        if self.dropout:\n",
    "            x = F.dropout2d(x, p=self.dropout)\n",
    "        return x\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        if self.dropout:\n",
    "            x = F.dropout2d(x, p=self.dropout)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "        return x\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            self.block(features * 4, features * 8),\n",
    "            self.block(features * 8, features * 8),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upblock(features * 8, features * 8),\n",
    "            self.upblock(features * 8 * 2, features * 4),\n",
    "            self.upblock(features * 4 * 2, features * 2),\n",
    "            self.upblock(features * 2 * 2, features),\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upblock(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        skips = skips[:-1][::-1]  # reverse except last\n",
    "        for idx, layer in enumerate(self.decoder[:-2]):\n",
    "            x = layer(x)\n",
    "            if idx < len(skips):\n",
    "                x = torch.cat([x, skips[idx]], 1)\n",
    "\n",
    "        return self.decoder[-2](x)\n",
    "\n",
    "\n",
    "# ========== 推論処理 ==========\n",
    "def load_generator(model_path, device):\n",
    "    model = UNetGenerator().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def denoise_and_correct(generator, input_path, output_path, device):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    img = Image.open(input_path).convert('RGB')\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)  # (1,3,H,W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_denoised = wavelet_denoise(input_tensor)\n",
    "        output = generator(input_denoised)\n",
    "        output_img = (output.squeeze(0).cpu() * 0.5) + 0.5  # [-1,1] -> [0,1]\n",
    "\n",
    "    output_pil = transforms.ToPILImage()(output_img.clamp(0,1))\n",
    "    output_pil.save(output_path)\n",
    "\n",
    "\n",
    "def wavelet_denoise_and_save(input_path, output_path, device):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    img = Image.open(input_path).convert('RGB')\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)  # (1,3,H,W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        denoised_tensor = wavelet_denoise(input_tensor)\n",
    "\n",
    "    # denoised_tensorは[-1,1]範囲なので[0,1]に変換\n",
    "    denoised_img = (denoised_tensor.squeeze(0).cpu() * 0.5) + 0.5\n",
    "    denoised_pil = transforms.ToPILImage()(denoised_img.clamp(0,1))\n",
    "    denoised_pil.save(output_path)\n",
    "\n",
    "    \n",
    "\n",
    "# ========== メイン処理 ==========\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator = load_generator(\"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\gen_weights_epoch100.pth\", device)\n",
    "\n",
    "    input_dir = \"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\testdata2\\\\renamed_FGSM\"\n",
    "    wavelet_dir = \"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\testdata2\\\\clean\"\n",
    "    output_dir = \"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\testdata2\\\\corrected\"\n",
    "    \n",
    "    os.makedirs(wavelet_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Waveletノイズ除去画像を別フォルダに保存\n",
    "    for fname in tqdm(os.listdir(input_dir)):\n",
    "        if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            continue\n",
    "        input_path = os.path.join(input_dir, fname)\n",
    "        wavelet_path = os.path.join(wavelet_dir, fname)\n",
    "        wavelet_denoise_and_save(input_path, wavelet_path, device)\n",
    "\n",
    "    # 2. GANで復元処理（Waveletノイズ除去画像を入力に）\n",
    "    for fname in tqdm(os.listdir(wavelet_dir)):\n",
    "        if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            continue\n",
    "        input_path = os.path.join(wavelet_dir, fname)\n",
    "        output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "        img = Image.open(input_path).convert('RGB')\n",
    "        input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = generator(input_tensor)\n",
    "            output_img = (output.squeeze(0).cpu() * 0.5) + 0.5  # [-1,1]→[0,1]\n",
    "\n",
    "        output_pil = transforms.ToPILImage()(output_img.clamp(0,1))\n",
    "        output_pil.save(output_path)\n",
    "\n",
    "    print(f\"Waveletノイズ除去画像を {wavelet_dir} に保存しました。\")\n",
    "    print(f\"復元画像を {output_dir} に保存しました。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c1fe0-44e2-426a-b1ea-818594abb14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba985f44-016e-4d65-9994-3eb58f3a12c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:24<00:00, 201.20it/s]\n"
     ]
    }
   ],
   "source": [
    "#元画像を256*256にするコード\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_dir =\"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\train\\\\normal\"\n",
    "output_dir = \"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\train\\\\normal256\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for fname in tqdm(os.listdir(input_dir)):\n",
    "    if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        continue\n",
    "    input_path = os.path.join(input_dir, fname)\n",
    "    output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    img = Image.open(input_path).convert(\"RGB\")\n",
    "    img_resized = img.resize((256, 256), Image.BILINEAR)  # リサイズ\n",
    "    img_resized.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6ab58b-705b-45df-a709-2dbbd7125c61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pix2pix_unet Epoch 1/50: 100%|██████████| 1250/1250 [00:32<00:00, 38.41it/s, D_loss=1.02, G_loss=7.11] \n",
      "pix2pix_unet Epoch 2/50: 100%|██████████| 1250/1250 [00:26<00:00, 47.02it/s, D_loss=0.0764, G_loss=7.17] \n",
      "pix2pix_unet Epoch 3/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.83it/s, D_loss=0.11, G_loss=7.99]  \n",
      "pix2pix_unet Epoch 4/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.54it/s, D_loss=0.0152, G_loss=9.01] \n",
      "pix2pix_unet Epoch 5/50: 100%|██████████| 1250/1250 [00:26<00:00, 47.07it/s, D_loss=0.666, G_loss=4.67]  \n",
      "pix2pix_unet Epoch 6/50: 100%|██████████| 1250/1250 [00:26<00:00, 47.07it/s, D_loss=0.685, G_loss=5.17]\n",
      "pix2pix_unet Epoch 7/50: 100%|██████████| 1250/1250 [00:26<00:00, 47.04it/s, D_loss=0.468, G_loss=6.3] \n",
      "pix2pix_unet Epoch 8/50: 100%|██████████| 1250/1250 [00:26<00:00, 47.02it/s, D_loss=0.308, G_loss=7.04] \n",
      "pix2pix_unet Epoch 9/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.89it/s, D_loss=1.04, G_loss=6.8]   \n",
      "pix2pix_unet Epoch 10/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.87it/s, D_loss=0.834, G_loss=4.98] \n",
      "pix2pix_unet Epoch 11/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.74it/s, D_loss=0.482, G_loss=7.75] \n",
      "pix2pix_unet Epoch 12/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.73it/s, D_loss=0.767, G_loss=4.73] \n",
      "pix2pix_unet Epoch 13/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.68it/s, D_loss=0.689, G_loss=5.19] \n",
      "pix2pix_unet Epoch 14/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.69it/s, D_loss=0.0719, G_loss=8.3] \n",
      "pix2pix_unet Epoch 15/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.82it/s, D_loss=0.0952, G_loss=9.91] \n",
      "pix2pix_unet Epoch 16/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.53it/s, D_loss=0.652, G_loss=5.66] \n",
      "pix2pix_unet Epoch 17/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.67it/s, D_loss=0.659, G_loss=6.15] \n",
      "pix2pix_unet Epoch 18/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.60it/s, D_loss=1.65, G_loss=8.04]  \n",
      "pix2pix_unet Epoch 19/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.66it/s, D_loss=0.248, G_loss=8.25] \n",
      "pix2pix_unet Epoch 20/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.75it/s, D_loss=0.224, G_loss=7.03] \n",
      "pix2pix_unet Epoch 21/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.82it/s, D_loss=0.362, G_loss=6.16] \n",
      "pix2pix_unet Epoch 22/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.56it/s, D_loss=0.2, G_loss=7.3]    \n",
      "pix2pix_unet Epoch 23/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.73it/s, D_loss=0.826, G_loss=4.59] \n",
      "pix2pix_unet Epoch 24/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.72it/s, D_loss=0.0817, G_loss=10.8] \n",
      "pix2pix_unet Epoch 25/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.71it/s, D_loss=0.142, G_loss=7.91] \n",
      "pix2pix_unet Epoch 26/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.79it/s, D_loss=0.236, G_loss=7.41]  \n",
      "pix2pix_unet Epoch 27/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.75it/s, D_loss=0.0737, G_loss=9.37]\n",
      "pix2pix_unet Epoch 28/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.71it/s, D_loss=0.201, G_loss=8.87]  \n",
      "pix2pix_unet Epoch 29/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.63it/s, D_loss=0.302, G_loss=7.29] \n",
      "pix2pix_unet Epoch 30/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.67it/s, D_loss=0.421, G_loss=6.13]  \n",
      "pix2pix_unet Epoch 31/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.62it/s, D_loss=0.102, G_loss=9.39] \n",
      "pix2pix_unet Epoch 32/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.68it/s, D_loss=0.0494, G_loss=10.3]\n",
      "pix2pix_unet Epoch 33/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.76it/s, D_loss=0.167, G_loss=7.21]  \n",
      "pix2pix_unet Epoch 34/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.72it/s, D_loss=0.372, G_loss=9.92] \n",
      "pix2pix_unet Epoch 35/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.70it/s, D_loss=0.259, G_loss=7.95] \n",
      "pix2pix_unet Epoch 36/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.85it/s, D_loss=0.0854, G_loss=10.3]\n",
      "pix2pix_unet Epoch 37/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.83it/s, D_loss=0.143, G_loss=8.01]  \n",
      "pix2pix_unet Epoch 38/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.68it/s, D_loss=0.319, G_loss=7.59] \n",
      "pix2pix_unet Epoch 39/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.92it/s, D_loss=0.149, G_loss=8.69] \n",
      "pix2pix_unet Epoch 40/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.80it/s, D_loss=0.0307, G_loss=9.62]\n",
      "pix2pix_unet Epoch 41/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.77it/s, D_loss=0.488, G_loss=8.14] \n",
      "pix2pix_unet Epoch 42/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.88it/s, D_loss=0.239, G_loss=9.06]  \n",
      "pix2pix_unet Epoch 43/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.70it/s, D_loss=0.0377, G_loss=9.84]\n",
      "pix2pix_unet Epoch 44/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.72it/s, D_loss=0.367, G_loss=8.3]  \n",
      "pix2pix_unet Epoch 45/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.75it/s, D_loss=0.562, G_loss=6.33] \n",
      "pix2pix_unet Epoch 46/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.68it/s, D_loss=0.209, G_loss=7.28] \n",
      "pix2pix_unet Epoch 47/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.71it/s, D_loss=0.405, G_loss=7.28] \n",
      "pix2pix_unet Epoch 48/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.80it/s, D_loss=0.0972, G_loss=8.06] \n",
      "pix2pix_unet Epoch 49/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.58it/s, D_loss=0.131, G_loss=9.03] \n",
      "pix2pix_unet Epoch 50/50: 100%|██████████| 1250/1250 [00:26<00:00, 46.66it/s, D_loss=0.102, G_loss=8.13] \n",
      "crunet_residual Epoch 1/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.30it/s, D_loss=0.111, G_loss=6.46] \n",
      "crunet_residual Epoch 2/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.15it/s, D_loss=0.0707, G_loss=12.1] \n",
      "crunet_residual Epoch 3/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.23it/s, D_loss=0.0812, G_loss=6.42] \n",
      "crunet_residual Epoch 4/50: 100%|██████████| 1250/1250 [00:24<00:00, 52.01it/s, D_loss=0.0406, G_loss=9.84]\n",
      "crunet_residual Epoch 5/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.32it/s, D_loss=0.0995, G_loss=8.36] \n",
      "crunet_residual Epoch 6/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.15it/s, D_loss=0.107, G_loss=8.7]  \n",
      "crunet_residual Epoch 7/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.29it/s, D_loss=0.118, G_loss=8.75]  \n",
      "crunet_residual Epoch 8/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.28it/s, D_loss=0.178, G_loss=7.6]  \n",
      "crunet_residual Epoch 9/50: 100%|██████████| 1250/1250 [00:24<00:00, 51.70it/s, D_loss=0.476, G_loss=7.38] \n",
      "crunet_residual Epoch 10/50: 100%|██████████| 1250/1250 [00:24<00:00, 51.84it/s, D_loss=0.322, G_loss=8.01] \n",
      "crunet_residual Epoch 11/50: 100%|██████████| 1250/1250 [00:24<00:00, 51.32it/s, D_loss=0.355, G_loss=7.76] \n",
      "crunet_residual Epoch 12/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.14it/s, D_loss=0.998, G_loss=6.95] \n",
      "crunet_residual Epoch 13/50: 100%|██████████| 1250/1250 [00:24<00:00, 51.96it/s, D_loss=0.146, G_loss=7.61] \n",
      "crunet_residual Epoch 14/50: 100%|██████████| 1250/1250 [00:25<00:00, 49.11it/s, D_loss=0.126, G_loss=8.96] \n",
      "crunet_residual Epoch 15/50: 100%|██████████| 1250/1250 [00:24<00:00, 51.98it/s, D_loss=0.0513, G_loss=9.33]\n",
      "crunet_residual Epoch 16/50: 100%|██████████| 1250/1250 [00:24<00:00, 51.98it/s, D_loss=1.48, G_loss=6.03]  \n",
      "crunet_residual Epoch 17/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.19it/s, D_loss=0.105, G_loss=7.34]  \n",
      "crunet_residual Epoch 18/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.11it/s, D_loss=0.101, G_loss=10.7] \n",
      "crunet_residual Epoch 19/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.10it/s, D_loss=0.107, G_loss=8.19] \n",
      "crunet_residual Epoch 20/50: 100%|██████████| 1250/1250 [00:24<00:00, 51.83it/s, D_loss=0.228, G_loss=5.88] \n",
      "crunet_residual Epoch 21/50: 100%|██████████| 1250/1250 [00:24<00:00, 51.84it/s, D_loss=0.14, G_loss=9.07]  \n",
      "crunet_residual Epoch 22/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.09it/s, D_loss=0.0792, G_loss=8.72]\n",
      "crunet_residual Epoch 23/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.29it/s, D_loss=0.231, G_loss=9.08] \n",
      "crunet_residual Epoch 24/50: 100%|██████████| 1250/1250 [00:24<00:00, 52.00it/s, D_loss=0.126, G_loss=8.44] \n",
      "crunet_residual Epoch 25/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.21it/s, D_loss=0.172, G_loss=6.95] \n",
      "crunet_residual Epoch 26/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.23it/s, D_loss=0.0587, G_loss=10.2]\n",
      "crunet_residual Epoch 27/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.21it/s, D_loss=0.0995, G_loss=8.73]\n",
      "crunet_residual Epoch 28/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.11it/s, D_loss=0.498, G_loss=7.44] \n",
      "crunet_residual Epoch 29/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.28it/s, D_loss=1.5, G_loss=5.03]   \n",
      "crunet_residual Epoch 30/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.65it/s, D_loss=0.319, G_loss=7.21] \n",
      "crunet_residual Epoch 31/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.69it/s, D_loss=0.837, G_loss=7.15] \n",
      "crunet_residual Epoch 32/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.90it/s, D_loss=0.191, G_loss=6.84] \n",
      "crunet_residual Epoch 33/50: 100%|██████████| 1250/1250 [00:23<00:00, 53.07it/s, D_loss=0.132, G_loss=8.57] \n",
      "crunet_residual Epoch 34/50: 100%|██████████| 1250/1250 [00:23<00:00, 53.48it/s, D_loss=0.677, G_loss=5.82] \n",
      "crunet_residual Epoch 35/50: 100%|██████████| 1250/1250 [00:23<00:00, 53.12it/s, D_loss=0.329, G_loss=5.38]\n",
      "crunet_residual Epoch 36/50: 100%|██████████| 1250/1250 [00:23<00:00, 53.45it/s, D_loss=0.61, G_loss=4.75] \n",
      "crunet_residual Epoch 37/50: 100%|██████████| 1250/1250 [00:23<00:00, 54.30it/s, D_loss=0.296, G_loss=8.48] \n",
      "crunet_residual Epoch 38/50: 100%|██████████| 1250/1250 [00:23<00:00, 54.23it/s, D_loss=0.359, G_loss=6.16]\n",
      "crunet_residual Epoch 39/50: 100%|██████████| 1250/1250 [00:23<00:00, 54.15it/s, D_loss=0.221, G_loss=6.54] \n",
      "crunet_residual Epoch 40/50: 100%|██████████| 1250/1250 [00:23<00:00, 54.05it/s, D_loss=0.28, G_loss=6.77] \n",
      "crunet_residual Epoch 41/50: 100%|██████████| 1250/1250 [00:24<00:00, 52.02it/s, D_loss=0.939, G_loss=4.35]\n",
      "crunet_residual Epoch 42/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.59it/s, D_loss=0.277, G_loss=5.65]\n",
      "crunet_residual Epoch 43/50: 100%|██████████| 1250/1250 [00:23<00:00, 53.49it/s, D_loss=0.406, G_loss=5.03]\n",
      "crunet_residual Epoch 44/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.85it/s, D_loss=0.138, G_loss=8.17] \n",
      "crunet_residual Epoch 45/50: 100%|██████████| 1250/1250 [00:23<00:00, 53.68it/s, D_loss=0.328, G_loss=6.68]\n",
      "crunet_residual Epoch 46/50: 100%|██████████| 1250/1250 [00:23<00:00, 53.96it/s, D_loss=0.588, G_loss=5.67]\n",
      "crunet_residual Epoch 47/50: 100%|██████████| 1250/1250 [00:23<00:00, 53.52it/s, D_loss=0.816, G_loss=3.8] \n",
      "crunet_residual Epoch 48/50: 100%|██████████| 1250/1250 [00:23<00:00, 53.56it/s, D_loss=0.266, G_loss=7.33]\n",
      "crunet_residual Epoch 49/50: 100%|██████████| 1250/1250 [00:23<00:00, 52.09it/s, D_loss=0.334, G_loss=6.51]\n",
      "crunet_residual Epoch 50/50: 100%|██████████| 1250/1250 [00:24<00:00, 51.47it/s, D_loss=0.232, G_loss=6.86]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===============================\n",
    "# Dataset\n",
    "# ===============================\n",
    "class AE2CleanDataset(Dataset):\n",
    "    def __init__(self, ae_dir, clean_dir, transform=None):\n",
    "        self.ae_dir = ae_dir\n",
    "        self.clean_dir = clean_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = sorted(os.listdir(ae_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ae_path = os.path.join(self.ae_dir, self.filenames[idx])\n",
    "        clean_path = os.path.join(self.clean_dir, self.filenames[idx])\n",
    "\n",
    "        ae_img = Image.open(ae_path).convert(\"RGB\")\n",
    "        clean_img = Image.open(clean_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            ae_img = self.transform(ae_img)\n",
    "            clean_img = self.transform(clean_img)\n",
    "\n",
    "        return ae_img, clean_img\n",
    "\n",
    "# ===============================\n",
    "# Pix2Pix Generator (U-Net)\n",
    "# ===============================\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.down1 = self.block(in_channels, features, normalize=False)\n",
    "        self.down2 = self.block(features, features*2)\n",
    "        self.down3 = self.block(features*2, features*4)\n",
    "        self.down4 = self.block(features*4, features*8)\n",
    "        self.down5 = self.block(features*8, features*8)\n",
    "\n",
    "        self.up1 = self.upblock(features*8, features*8)\n",
    "        self.up2 = self.upblock(features*8*2, features*4)\n",
    "        self.up3 = self.upblock(features*4*2, features*2)\n",
    "        self.up4 = self.upblock(features*2*2, features)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2, out_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upblock(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        bottleneck = self.down5(d4)\n",
    "\n",
    "        u1 = self.up1(bottleneck)\n",
    "        u1 = torch.cat([u1, d4], 1)\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat([u2, d3], 1)\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = torch.cat([u3, d2], 1)\n",
    "        u4 = self.up4(u3)\n",
    "        u4 = torch.cat([u4, d1], 1)\n",
    "        return self.final(u4)\n",
    "\n",
    "# ===============================\n",
    "# CRU-Net Generator\n",
    "# ===============================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(x + self.block(x), -1, 1)\n",
    "\n",
    "class CRUNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(features),\n",
    "            nn.Conv2d(features, features*2, 4, 2, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(features*2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2, features, 4, 2, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(features),\n",
    "            nn.ConvTranspose2d(features, out_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        # Generator outputs residual noise\n",
    "        return torch.clamp(x + out, -1, 1)\n",
    "\n",
    "# ===============================\n",
    "# Patch Discriminator\n",
    "# ===============================\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=6, features=64):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features*2),\n",
    "            self.block(features*2, features*4),\n",
    "            nn.Conv2d(features*4, 1, 4, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x, y):\n",
    "        return self.model(torch.cat([x, y], 1))\n",
    "\n",
    "# ===============================\n",
    "# Training Loop\n",
    "# ===============================\n",
    "def train_model(model_name, generator_class, ae_dir, clean_dir, epochs=50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    dataset = AE2CleanDataset(ae_dir, clean_dir, transform)\n",
    "    loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    gen = generator_class().to(device)\n",
    "    disc = PatchDiscriminator().to(device)\n",
    "    opt_g = torch.optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    opt_d = torch.optim.Adam(disc.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    bce = nn.BCELoss()\n",
    "    l1 = nn.L1Loss()\n",
    "\n",
    "    os.makedirs(f\"./outputs_{model_name}\", exist_ok=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(loader, desc=f\"{model_name} Epoch {epoch+1}/{epochs}\")\n",
    "        for ae_img, clean_img in loop:\n",
    "            ae_img, clean_img = ae_img.to(device), clean_img.to(device)\n",
    "\n",
    "            # --- Discriminator ---\n",
    "            fake_img = gen(ae_img).detach()\n",
    "            real_pred = disc(ae_img, clean_img)\n",
    "            fake_pred = disc(ae_img, fake_img)\n",
    "            d_loss = (bce(real_pred, torch.ones_like(real_pred)) +\n",
    "                      bce(fake_pred, torch.zeros_like(fake_pred))) / 2\n",
    "\n",
    "            opt_d.zero_grad()\n",
    "            d_loss.backward()\n",
    "            opt_d.step()\n",
    "\n",
    "            # --- Generator ---\n",
    "            fake_img = gen(ae_img)\n",
    "            pred = disc(ae_img, fake_img)\n",
    "            adv_loss = bce(pred, torch.ones_like(pred))\n",
    "            l1_loss = l1(fake_img, clean_img) * 100\n",
    "            g_loss = adv_loss + l1_loss\n",
    "\n",
    "            opt_g.zero_grad()\n",
    "            g_loss.backward()\n",
    "            opt_g.step()\n",
    "\n",
    "            loop.set_postfix(G_loss=g_loss.item(), D_loss=d_loss.item())\n",
    "\n",
    "        # Save outputs every 10 epochs\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            save_image(fake_img * 0.5 + 0.5, f\"./outputs_{model_name}/fake_epoch{epoch+1}.png\")\n",
    "            torch.save(gen.state_dict(), f\"./outputs_{model_name}/gen_epoch{epoch+1}.pth\")\n",
    "\n",
    "# ===============================\n",
    "# Main\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    ae_dir = \"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\train\\\\AE_wavelet\"\n",
    "    clean_dir = \"C:\\\\Users\\\\sit\\\\wavelet_CGAN\\\\train\\\\normal256\"\n",
    "\n",
    "    train_model(\"pix2pix_unet\", UNetGenerator, ae_dir, clean_dir, epochs=50)\n",
    "    train_model(\"crunet_residual\", CRUNetGenerator, ae_dir, clean_dir, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74f346-2870-4d4e-829a-63b1bcf4232f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch260)",
   "language": "python",
   "name": "torch260"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
