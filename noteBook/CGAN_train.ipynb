{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "818f314d-9708-48b6-921f-daef4c07d5f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CGAN (pix2pix) å­¦ç¿’é–‹å§‹ ===\n",
      "å…¥åŠ›: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data1_FGSM\\wavelet_eval\\wavelet_tensor\n",
      "æ­£è§£: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data1_clean_tensor\n",
      "å‡ºåŠ›: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 23.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 1 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 25.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 2 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 3 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 4 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 5 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 6 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 7 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 8 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 9 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 10 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 11 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 12 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 13 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 14 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 15 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 16 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 17 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 18 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 19 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 20 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 21 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 22 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 23 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 24 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 25 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 26 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 27 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 28 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 29 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 30 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 31 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 32 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 33 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 34 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 35 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 36 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 37 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 38 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 39 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 40 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 41 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 42 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 43 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 44 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 45 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 46 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 47 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 48 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 49 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 50 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 51 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 52 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 53 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 54 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 55 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 56 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 57 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 58 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 59 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 60 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 61 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 62 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 63 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 64 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 65 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 66 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 67 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 68 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 69 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 70 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 71 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 72 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 73 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 74 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 75 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 76 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 77 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 78 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 79 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 80 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 81 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 82 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 83 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 84 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:04<00:00, 25.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 85 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 86 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 87 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 88 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 89 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 90 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 91 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 92 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 93 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 94 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 95 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 96 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 97 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 98 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 99 å®Œäº†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 24.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 100 å®Œäº†\n",
      "\n",
      "=== å­¦ç¿’å®Œäº† ===\n",
      "â± ç·å­¦ç¿’æ™‚é–“: 8.75 åˆ†\n",
      "ç”Ÿæˆç”»åƒ: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_train\\images\n",
      "ãƒ¢ãƒ‡ãƒ«é‡ã¿: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_train\\weights\n",
      "ãƒ­ã‚°: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_train\\logs\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# CGAN (pix2pix) å­¦ç¿’: Wavelet Tensor â†’ Clean Tensor\n",
    "# æ”¹è‰¯ç‰ˆï¼šå‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆ + å­¦ç¿’æ™‚é–“æ¸¬å®š + Lossã‚°ãƒ©ãƒ•ä¿å­˜æ©Ÿèƒ½ä»˜ã\n",
    "# ==========================================================\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === Dataset (Tensorå…¥åŠ›å¯¾å¿œ) ===\n",
    "# ==========================================================\n",
    "class AE2CleanTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    AE (æ•µå¯¾çš„ã‚µãƒ³ãƒ—ãƒ«Tensor) ã¨ Clean (æ­£è§£ç”»åƒTensor) ã®ãƒšã‚¢ã‚’èª­ã¿è¾¼ã‚€Datasetã€‚\n",
    "    å€¤ãŒ [0,1] ã®å ´åˆã¯è‡ªå‹•ã§ [-1,1] ã«æ­£è¦åŒ–ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, ae_dir, clean_dir, auto_normalize=True):\n",
    "        self.ae_dir = ae_dir\n",
    "        self.clean_dir = clean_dir\n",
    "        self.auto_normalize = auto_normalize\n",
    "        self.filenames = sorted(os.listdir(ae_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ae_path = os.path.join(self.ae_dir, self.filenames[idx])\n",
    "        clean_path = os.path.join(self.clean_dir, self.filenames[idx])\n",
    "\n",
    "        ae_tensor = torch.load(ae_path)\n",
    "        clean_tensor = torch.load(clean_path)\n",
    "\n",
    "        if ae_tensor.ndim == 4:\n",
    "            ae_tensor = ae_tensor.squeeze(0)\n",
    "        if clean_tensor.ndim == 4:\n",
    "            clean_tensor = clean_tensor.squeeze(0)\n",
    "\n",
    "        if ae_tensor.shape[0] == 1:\n",
    "            ae_tensor = ae_tensor.repeat(3, 1, 1)\n",
    "        if clean_tensor.shape[0] == 1:\n",
    "            clean_tensor = clean_tensor.repeat(3, 1, 1)\n",
    "\n",
    "        if self.auto_normalize:\n",
    "            if ae_tensor.min() >= 0.0 and ae_tensor.max() <= 1.0:\n",
    "                ae_tensor = (ae_tensor - 0.5) / 0.5\n",
    "            if clean_tensor.min() >= 0.0 and clean_tensor.max() <= 1.0:\n",
    "                clean_tensor = (clean_tensor - 0.5) / 0.5\n",
    "\n",
    "        return ae_tensor, clean_tensor\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === Generator (U-Netæ§‹é€ ) ===\n",
    "# ==========================================================\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            self.block(features * 4, features * 8),\n",
    "            self.block(features * 8, features * 8),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upblock(features * 8, features * 8),\n",
    "            self.upblock(features * 8 * 2, features * 4),\n",
    "            self.upblock(features * 4 * 2, features * 2),\n",
    "            self.upblock(features * 2 * 2, features),\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upblock(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "        skips = skips[:-1][::-1]\n",
    "        for idx, layer in enumerate(self.decoder[:-2]):\n",
    "            x = layer(x)\n",
    "            if idx < len(skips):\n",
    "                x = torch.cat([x, skips[idx]], 1)\n",
    "        return self.decoder[-2](x)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === Discriminator (PatchGAN) ===\n",
    "# ==========================================================\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=6, features=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            nn.Conv2d(features * 4, 1, kernel_size=4, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.net(torch.cat([x, y], 1))\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === å­¦ç¿’ãƒ«ãƒ¼ãƒ— ===\n",
    "# ==========================================================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ae_dir = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data1_FGSM\\wavelet_eval\\wavelet_tensor\"\n",
    "    clean_dir = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data1_clean_tensor\"\n",
    "\n",
    "    save_root = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_train\"\n",
    "    save_img_dir = os.path.join(save_root, \"images\")\n",
    "    save_weight_dir = os.path.join(save_root, \"weights\")\n",
    "    save_log_dir = os.path.join(save_root, \"logs\")\n",
    "    os.makedirs(save_img_dir, exist_ok=True)\n",
    "    os.makedirs(save_weight_dir, exist_ok=True)\n",
    "    os.makedirs(save_log_dir, exist_ok=True)\n",
    "\n",
    "    dataset = AE2CleanTensorDataset(ae_dir, clean_dir)\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    gen = UNetGenerator().to(device)\n",
    "    disc = PatchDiscriminator().to(device)\n",
    "\n",
    "    opt_g = torch.optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    opt_d = torch.optim.Adam(disc.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    bce = nn.BCELoss()\n",
    "    l1 = nn.L1Loss()\n",
    "\n",
    "    G_loss_list, D_loss_list = [], []\n",
    "\n",
    "    print(f\"\\n=== CGAN (pix2pix) å­¦ç¿’é–‹å§‹ ===\")\n",
    "    print(f\"å…¥åŠ›: {ae_dir}\")\n",
    "    print(f\"æ­£è§£: {clean_dir}\")\n",
    "    print(f\"å‡ºåŠ›: {save_root}\\n\")\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch+1}/100\")\n",
    "        epoch_g_loss, epoch_d_loss = 0, 0\n",
    "\n",
    "        for ae_tensor, clean_tensor in loop:\n",
    "            ae_tensor, clean_tensor = ae_tensor.to(device), clean_tensor.to(device)\n",
    "\n",
    "            fake_img = gen(ae_tensor).detach()\n",
    "            real_pred = disc(ae_tensor, clean_tensor)\n",
    "            fake_pred = disc(ae_tensor, fake_img)\n",
    "            real_loss = bce(real_pred, torch.ones_like(real_pred))\n",
    "            fake_loss = bce(fake_pred, torch.zeros_like(fake_pred))\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            opt_d.zero_grad()\n",
    "            d_loss.backward()\n",
    "            opt_d.step()\n",
    "\n",
    "            fake_img = gen(ae_tensor)\n",
    "            pred = disc(ae_tensor, fake_img)\n",
    "            adv_loss = bce(pred, torch.ones_like(pred))\n",
    "            l1_loss_val = l1(fake_img, clean_tensor) * 100\n",
    "            g_loss = adv_loss + l1_loss_val\n",
    "            opt_g.zero_grad()\n",
    "            g_loss.backward()\n",
    "            opt_g.step()\n",
    "\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "\n",
    "        G_loss_list.append(epoch_g_loss / len(loader))\n",
    "        D_loss_list.append(epoch_d_loss / len(loader))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample = (fake_img * 0.5 + 0.5).clamp(0, 1)\n",
    "            save_image(sample, os.path.join(save_img_dir, f\"fake_epoch{epoch+1}.png\"))\n",
    "\n",
    "        torch.save(gen.state_dict(), os.path.join(save_weight_dir, f\"gen_weights_epoch{epoch+1}.pth\"))\n",
    "\n",
    "        print(f\"ðŸ•’ Epoch {epoch+1} å®Œäº†\")\n",
    "\n",
    "    df_log = pd.DataFrame({\"Epoch\": range(1, 101), \"G_loss\": G_loss_list, \"D_loss\": D_loss_list})\n",
    "    df_log.to_csv(os.path.join(save_log_dir, \"loss_history.csv\"), index=False)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(G_loss_list, label=\"Generator Loss\")\n",
    "    plt.plot(D_loss_list, label=\"Discriminator Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Curve (100 Epoch)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_log_dir, \"loss_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\n=== å­¦ç¿’å®Œäº† ===\")\n",
    "    print(f\"â± ç·å­¦ç¿’æ™‚é–“: {total_time/60:.2f} åˆ†\")\n",
    "    print(f\"ç”Ÿæˆç”»åƒ: {save_img_dir}\")\n",
    "    print(f\"ãƒ¢ãƒ‡ãƒ«é‡ã¿: {save_weight_dir}\")\n",
    "    print(f\"ãƒ­ã‚°: {save_log_dir}\")\n",
    "    print(\"==========================================================\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959e141a-236a-4cc3-8902-4f9625f6f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CGANæŽ¨è«–ï¼‹åˆ†é¡žé–‹å§‹ ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:45<00:00, 22.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… å¾©å…ƒï¼†åˆ†é¡žå®Œäº†\n",
      "å‡ºåŠ›ç”»åƒãƒ•ã‚©ãƒ«ãƒ€: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore\n",
      "åˆ†é¡žçµæžœCSV: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore\\pix2pix_class_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# CGAN (pix2pix) æŽ¨è«– + åˆ†é¡žçµæžœCSVå‡ºåŠ›\n",
    "# ==========================================================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# --- MobileNetV2 åˆ†é¡žå™¨ãƒ­ãƒ¼ãƒ‰ ---\n",
    "def load_classifier(device):\n",
    "    model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "    model = model.to(device).eval()\n",
    "    categories = models.MobileNet_V2_Weights.IMAGENET1K_V1.meta[\"categories\"]\n",
    "    normalize = models.MobileNet_V2_Weights.IMAGENET1K_V1.transforms()\n",
    "    return model, normalize, categories\n",
    "\n",
    "\n",
    "# --- Generator (U-Net) ---\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            self.block(features * 4, features * 8),\n",
    "            self.block(features * 8, features * 8),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upblock(features * 8, features * 8),\n",
    "            self.upblock(features * 8 * 2, features * 4),\n",
    "            self.upblock(features * 4 * 2, features * 2),\n",
    "            self.upblock(features * 2 * 2, features),\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upblock(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "        skips = skips[:-1][::-1]\n",
    "        for idx, layer in enumerate(self.decoder[:-2]):\n",
    "            x = layer(x)\n",
    "            if idx < len(skips):\n",
    "                x = torch.cat([x, skips[idx]], 1)\n",
    "        return self.decoder[-2](x)\n",
    "\n",
    "\n",
    "# --- å¾©å…ƒï¼‹åˆ†é¡žå‡¦ç† ---\n",
    "def restore_and_classify():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    input_dir = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data3_CW\\wavelet_eval\\wavelet_tensor\"\n",
    "    output_dir = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore\"\n",
    "    weight_path = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_train\\weights\\gen_weights_epoch100.pth\"\n",
    "    csv_path = os.path.join(output_dir, \"pix2pix_class_results.csv\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Generatorãƒ­ãƒ¼ãƒ‰ ---\n",
    "    gen = UNetGenerator().to(device)\n",
    "    gen.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    gen.eval()\n",
    "\n",
    "    # --- åˆ†é¡žãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ ---\n",
    "    clf, normalize, categories = load_classifier(device)\n",
    "\n",
    "    results = []\n",
    "    print(f\"\\n=== CGANæŽ¨è«–ï¼‹åˆ†é¡žé–‹å§‹ ===\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for fname in tqdm(sorted(os.listdir(input_dir))):\n",
    "            if not fname.endswith(\".pt\"):\n",
    "                continue\n",
    "\n",
    "            tensor = torch.load(os.path.join(input_dir, fname), map_location=device)\n",
    "            if tensor.min() >= 0.0 and tensor.max() <= 1.0:\n",
    "                tensor = (tensor - 0.5) / 0.5\n",
    "            if tensor.ndim == 3:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "\n",
    "            # --- CGANã«ã‚ˆã‚‹å¾©å…ƒ ---\n",
    "            restored = gen(tensor.to(device))\n",
    "            restored_01 = (restored * 0.5 + 0.5).clamp(0, 1)\n",
    "\n",
    "            # --- åˆ†é¡ž ---\n",
    "            normed = normalize(restored_01.squeeze(0))\n",
    "            preds = F.softmax(clf(normed.unsqueeze(0)), dim=1)\n",
    "            conf, idx = preds.max(1)\n",
    "            label = categories[int(idx)]\n",
    "            confidence = float(conf.item())\n",
    "\n",
    "            # --- ä¿å­˜ ---\n",
    "            save_path = os.path.join(output_dir, fname.replace(\".pt\", \"_restored.png\"))\n",
    "            save_image(restored_01.cpu(), save_path)\n",
    "\n",
    "            results.append({\n",
    "                \"ImageId\": os.path.splitext(fname)[0],\n",
    "                \"CGAN_Label\": label,\n",
    "                \"CGAN_Confidence\": round(confidence, 6),\n",
    "                \"Restored_Path\": save_path\n",
    "            })\n",
    "\n",
    "    # --- CSVå‡ºåŠ› ---\n",
    "    pd.DataFrame(results).to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"\\nâœ… å¾©å…ƒï¼†åˆ†é¡žå®Œäº†\")\n",
    "    print(f\"å‡ºåŠ›ç”»åƒãƒ•ã‚©ãƒ«ãƒ€: {output_dir}\")\n",
    "    print(f\"åˆ†é¡žçµæžœCSV: {csv_path}\")\n",
    "\n",
    "\n",
    "# --- å®Ÿè¡Œ ---\n",
    "if __name__ == \"__main__\":\n",
    "    restore_and_classify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9d99dc-9c89-4eca-b4a7-26c18541d33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 4æ®µéšŽCSVçµ±åˆï¼ˆWavelet+CGANå¯¾å¿œï¼‰ã‚’é–‹å§‹ ===\n",
      "\n",
      "æ”»æ’ƒå‰: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data3\\clean_dataset_labels.csv\n",
      "æ”»æ’ƒå¾Œ: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data3_CW\\CW_Results.csv\n",
      "åˆ†é¡žå›žå¾©å¾Œ(Wavelet): C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data3_CW\\wavelet_eval\\CW_Wavelet_class_results.csv\n",
      "åˆ†é¡žå›žå¾©å¾Œ(CGAN): C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore\\pix2pix_class_results.csv\n",
      "\n",
      "\n",
      "=== çµ±è¨ˆçµæžœ ===\n",
      "æ”»æ’ƒæˆåŠŸæ•°: 1000/1000 (100.00%)\n",
      "WaveletçŸ¯æ­£æˆåŠŸæ•°: 917/1000 (91.70%)\n",
      "CGANçŸ¯æ­£æˆåŠŸæ•°: 937/1000 (93.70%)\n",
      "\n",
      "âœ… å‡ºåŠ›å®Œäº†: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\compare_CW_Wavelet_CGAN.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# æ”»æ’ƒå‰ï¼‹æ”»æ’ƒå¾Œï¼‹Waveletï¼‹CGANæ¯”è¼ƒï¼†æˆåŠŸçŽ‡åˆ¤å®šï¼ˆå®Œå…¨ç‰ˆï¼‰\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== 4æ®µéšŽCSVçµ±åˆï¼ˆWavelet+CGANå¯¾å¿œï¼‰ã‚’é–‹å§‹ ===\\n\")\n",
    "\n",
    "# ==========================================================\n",
    "# === å„CSVãƒ‘ã‚¹ ===\n",
    "# ==========================================================\n",
    "before_csv = Path(r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data3\\clean_dataset_labels.csv\")\n",
    "after_csv = Path(r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data3_CW\\CW_Results.csv\")\n",
    "wavelet_csv = Path(r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data3_CW\\wavelet_eval\\CW_Wavelet_class_results.csv\")\n",
    "cgan_csv = Path(r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore\\pix2pix_class_results.csv\")\n",
    "\n",
    "# --- å‡ºåŠ›å…ˆ ---\n",
    "output_csv = Path(r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\compare_CW_Wavelet_CGAN.csv\")\n",
    "\n",
    "# ==========================================================\n",
    "# === CSVèª­ã¿è¾¼ã¿ ===\n",
    "# ==========================================================\n",
    "print(f\"æ”»æ’ƒå‰: {before_csv}\")\n",
    "print(f\"æ”»æ’ƒå¾Œ: {after_csv}\")\n",
    "print(f\"åˆ†é¡žå›žå¾©å¾Œ(Wavelet): {wavelet_csv}\")\n",
    "print(f\"åˆ†é¡žå›žå¾©å¾Œ(CGAN): {cgan_csv}\\n\")\n",
    "\n",
    "df_before = pd.read_csv(before_csv)\n",
    "df_after = pd.read_csv(after_csv)\n",
    "df_wave = pd.read_csv(wavelet_csv)\n",
    "df_cgan = pd.read_csv(cgan_csv)\n",
    "\n",
    "# ==========================================================\n",
    "# === ImageIdæ•´å½¢ ===\n",
    "# ==========================================================\n",
    "df_before[\"ImageId\"] = df_before[\"image\"].apply(lambda x: Path(str(x)).stem)\n",
    "df_after[\"ImageId\"] = df_after[\"image\"].apply(lambda x: Path(str(x)).stem)\n",
    "\n",
    "# âœ… CGANå´ã®ãƒ•ã‚¡ã‚¤ãƒ«åä¿®æ­£\n",
    "# ï¼ˆ_wavelet ã‚„ _restored ã®ã¤ã„ãŸãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚å¯¾å¿œï¼‰\n",
    "df_cgan[\"ImageId\"] = df_cgan[\"ImageId\"].apply(\n",
    "    lambda x: Path(str(x)).stem.replace(\"_wavelet_restored\", \"\").replace(\"_wavelet\", \"\").replace(\"_restored\", \"\")\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# === åˆ—åçµ±ä¸€ ===\n",
    "# ==========================================================\n",
    "df_before = df_before.rename(columns={\n",
    "    \"label_name\": \"label_name_before\",\n",
    "    \"confidence\": \"confidence_before\"\n",
    "})\n",
    "df_after = df_after.rename(columns={\n",
    "    \"Advs_label_name\": \"label_name_after\",\n",
    "    \"Advs_confidence\": \"confidence_after\"\n",
    "})\n",
    "\n",
    "# ==========================================================\n",
    "# === çµåˆå‡¦ç† ===\n",
    "# ==========================================================\n",
    "df_merged = (\n",
    "    df_before\n",
    "    .merge(df_after, on=\"ImageId\", suffixes=(\"_before\", \"_after\"))\n",
    "    .merge(df_wave, on=\"ImageId\", how=\"left\")\n",
    "    .merge(df_cgan, on=\"ImageId\", how=\"left\")\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# === æˆåŠŸåˆ¤å®š ===\n",
    "# ==========================================================\n",
    "df_merged[\"AttackSuccess\"] = df_merged[\"label_name_before\"] != df_merged[\"label_name_after\"]\n",
    "df_merged[\"RecoverySuccess_Wavelet\"] = df_merged[\"label_name_before\"] == df_merged[\"Wavelet_Label\"]\n",
    "df_merged[\"RecoverySuccess_CGAN\"] = df_merged[\"label_name_before\"] == df_merged[\"CGAN_Label\"]\n",
    "\n",
    "# ==========================================================\n",
    "# === å‡ºåŠ›æ•´å½¢ ===\n",
    "# ==========================================================\n",
    "cols = [\n",
    "    \"ImageId\",\n",
    "    \"label_name_before\", \"confidence_before\",\n",
    "    \"label_name_after\", \"confidence_after\",\n",
    "    \"Wavelet_Label\", \"Wavelet_Confidence\",\n",
    "    \"CGAN_Label\", \"CGAN_Confidence\",\n",
    "    \"AttackSuccess\",\n",
    "    \"RecoverySuccess_Wavelet\",\n",
    "    \"RecoverySuccess_CGAN\"\n",
    "]\n",
    "df_final = df_merged[cols]\n",
    "\n",
    "# ==========================================================\n",
    "# === çµæžœå‡ºåŠ› ===\n",
    "# ==========================================================\n",
    "df_final.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ==========================================================\n",
    "# === çµ±è¨ˆ ===\n",
    "# ==========================================================\n",
    "total = len(df_final)\n",
    "atk_success = df_final[\"AttackSuccess\"].sum()\n",
    "wavelet_ok = df_final[\"RecoverySuccess_Wavelet\"].sum()\n",
    "cgan_ok = df_final[\"RecoverySuccess_CGAN\"].sum()\n",
    "\n",
    "print(\"\\n=== çµ±è¨ˆçµæžœ ===\")\n",
    "print(f\"æ”»æ’ƒæˆåŠŸæ•°: {atk_success}/{total} ({atk_success/total*100:.2f}%)\")\n",
    "print(f\"WaveletçŸ¯æ­£æˆåŠŸæ•°: {wavelet_ok}/{total} ({wavelet_ok/total*100:.2f}%)\")\n",
    "print(f\"CGANçŸ¯æ­£æˆåŠŸæ•°: {cgan_ok}/{total} ({cgan_ok/total*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… å‡ºåŠ›å®Œäº†: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a85c00-d95a-4754-b054-55e9c2df35bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CGAN (pix2pix) å­¦ç¿’é–‹å§‹ [CPUãƒ¢ãƒ¼ãƒ‰ / GPUåŒæ¡ä»¶] ===\n",
      "å…¥åŠ›: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data1_FGSM\\tensor\n",
      "æ­£è§£: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data1_clean_tensor\n",
      "å‡ºåŠ›: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_train_CPU_samecond\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:44<00:00,  1.20it/s, D_loss=0.488, G_loss=14.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 1 å®Œäº†: 1.74 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.542, G_loss=7.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 2 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.15, G_loss=8.86]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 3 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.0689, G_loss=8.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 4 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=2.33, G_loss=6.42]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 5 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.0419, G_loss=7.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 6 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.0142, G_loss=9.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 7 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.00719, G_loss=11.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 8 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.0673, G_loss=8.56] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 9 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.112, G_loss=9.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 10 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.0205, G_loss=10.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 11 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.0203, G_loss=10.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 12 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=1.58, G_loss=5.7]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 13 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.0284, G_loss=7.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 14 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.308, G_loss=7.83] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 15 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.0552, G_loss=7.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 16 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:42<00:00,  1.22it/s, D_loss=2.51, G_loss=4.53]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 17 å®Œäº†: 1.71 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:42<00:00,  1.22it/s, D_loss=0.636, G_loss=2.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 18 å®Œäº†: 1.71 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.145, G_loss=4.92] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 19 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.653, G_loss=5.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 20 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.6, G_loss=3.77]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 21 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.641, G_loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 22 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.0547, G_loss=5.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 23 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=1.1, G_loss=3.94]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 24 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.687, G_loss=3.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 25 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.64, G_loss=2.89] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 26 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.598, G_loss=2.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 27 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.24it/s, D_loss=0.683, G_loss=2.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 28 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.52, G_loss=4.03]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 29 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.0938, G_loss=4.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 30 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.128, G_loss=5.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 31 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.11, G_loss=4.16]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 32 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=1.12, G_loss=4.35]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 33 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.205, G_loss=4.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 34 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.77, G_loss=2.86]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 35 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=1.16, G_loss=4.01]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 36 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:42<00:00,  1.22it/s, D_loss=0.0813, G_loss=4.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 37 å®Œäº†: 1.71 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:42<00:00,  1.22it/s, D_loss=0.636, G_loss=2.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 38 å®Œäº†: 1.71 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:42<00:00,  1.21it/s, D_loss=0.511, G_loss=2.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 39 å®Œäº†: 1.72 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.703, G_loss=2.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 40 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.126, G_loss=4.09] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 41 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.722, G_loss=5.16] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 42 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.622, G_loss=2.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 43 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.698, G_loss=2.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 44 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.701, G_loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 45 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.697, G_loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 46 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.24it/s, D_loss=0.703, G_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 47 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.702, G_loss=3.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 48 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.688, G_loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 49 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.679, G_loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 50 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.704, G_loss=3.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 51 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.24it/s, D_loss=0.754, G_loss=2.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 52 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.662, G_loss=2.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 53 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.683, G_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 54 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.677, G_loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 55 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.682, G_loss=2.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 56 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.927, G_loss=4.86] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 57 å®Œäº†: 1.69 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.073, G_loss=4.54] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 58 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [01:41<00:00,  1.23it/s, D_loss=0.467, G_loss=6.64] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•’ Epoch 59 å®Œäº†: 1.70 åˆ†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100:  20%|â–ˆâ–ˆ        | 25/125 [00:20<01:21,  1.23it/s, D_loss=0.102, G_loss=4.01] "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# CGAN (pix2pix) å­¦ç¿’: Wavelet Tensor â†’ Clean Tensor\n",
    "# CPUå›ºå®šç‰ˆï¼ˆGPUå®Ÿé¨“ã¨åŒæ¡ä»¶ãƒ»å­¦ç¿’æ™‚é–“æ¸¬å®šä»˜ãï¼‰\n",
    "# ==========================================================\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === Dataset (Tensorå…¥åŠ›å¯¾å¿œ) ===\n",
    "# ==========================================================\n",
    "class AE2CleanTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    AE (æ•µå¯¾çš„ã‚µãƒ³ãƒ—ãƒ«Tensor) ã¨ Clean (æ­£è§£ç”»åƒTensor) ã®ãƒšã‚¢ã‚’èª­ã¿è¾¼ã‚€Datasetã€‚\n",
    "    å€¤ãŒ [0,1] ã®å ´åˆã¯è‡ªå‹•ã§ [-1,1] ã«æ­£è¦åŒ–ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, ae_dir, clean_dir, auto_normalize=True):\n",
    "        self.ae_dir = ae_dir\n",
    "        self.clean_dir = clean_dir\n",
    "        self.auto_normalize = auto_normalize\n",
    "        self.filenames = sorted(os.listdir(ae_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ae_path = os.path.join(self.ae_dir, self.filenames[idx])\n",
    "        clean_path = os.path.join(self.clean_dir, self.filenames[idx])\n",
    "\n",
    "        ae_tensor = torch.load(ae_path, map_location=\"cpu\")\n",
    "        clean_tensor = torch.load(clean_path, map_location=\"cpu\")\n",
    "\n",
    "        # shapeæ•´å½¢\n",
    "        if ae_tensor.ndim == 4:\n",
    "            ae_tensor = ae_tensor.squeeze(0)\n",
    "        if clean_tensor.ndim == 4:\n",
    "            clean_tensor = clean_tensor.squeeze(0)\n",
    "\n",
    "        # 1ch â†’ RGBåŒ–\n",
    "        if ae_tensor.shape[0] == 1:\n",
    "            ae_tensor = ae_tensor.repeat(3, 1, 1)\n",
    "        if clean_tensor.shape[0] == 1:\n",
    "            clean_tensor = clean_tensor.repeat(3, 1, 1)\n",
    "\n",
    "        # æ­£è¦åŒ– [-1,1]\n",
    "        if self.auto_normalize:\n",
    "            if ae_tensor.min() >= 0.0 and ae_tensor.max() <= 1.0:\n",
    "                ae_tensor = (ae_tensor - 0.5) / 0.5\n",
    "            if clean_tensor.min() >= 0.0 and clean_tensor.max() <= 1.0:\n",
    "                clean_tensor = (clean_tensor - 0.5) / 0.5\n",
    "\n",
    "        return ae_tensor, clean_tensor\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === Generator (U-Netæ§‹é€ ) ===\n",
    "# ==========================================================\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            self.block(features * 4, features * 8),\n",
    "            self.block(features * 8, features * 8),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upblock(features * 8, features * 8),\n",
    "            self.upblock(features * 8 * 2, features * 4),\n",
    "            self.upblock(features * 4 * 2, features * 2),\n",
    "            self.upblock(features * 2 * 2, features),\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upblock(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "        skips = skips[:-1][::-1]\n",
    "        for idx, layer in enumerate(self.decoder[:-2]):\n",
    "            x = layer(x)\n",
    "            if idx < len(skips):\n",
    "                x = torch.cat([x, skips[idx]], 1)\n",
    "        return self.decoder[-2](x)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === Discriminator (PatchGAN) ===\n",
    "# ==========================================================\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=6, features=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            nn.Conv2d(features * 4, 1, kernel_size=4, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.net(torch.cat([x, y], 1))\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆCPUå›ºå®šãƒ»GPUç‰ˆã¨åŒæ¡ä»¶ï¼‰ ===\n",
    "# ==========================================================\n",
    "def train_cpu():\n",
    "    device = torch.device(\"cpu\")  # âœ… CPUå›ºå®š\n",
    "\n",
    "    # === ãƒ‘ã‚¹è¨­å®š ===\n",
    "    ae_dir = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data1_FGSM\\tensor\"\n",
    "    clean_dir = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data1_clean_tensor\"\n",
    "\n",
    "    # === å‡ºåŠ›å…ˆæ§‹æˆ ===\n",
    "    save_root = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_train_CPU_samecond\"\n",
    "    save_img_dir = os.path.join(save_root, \"images\")\n",
    "    save_weight_dir = os.path.join(save_root, \"weights\")\n",
    "    os.makedirs(save_img_dir, exist_ok=True)\n",
    "    os.makedirs(save_weight_dir, exist_ok=True)\n",
    "\n",
    "    # === ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ ===\n",
    "    dataset = AE2CleanTensorDataset(ae_dir, clean_dir)\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=True)  # âœ… GPUç‰ˆã¨åŒæ¡ä»¶\n",
    "\n",
    "    # === ãƒ¢ãƒ‡ãƒ«å®šç¾© ===\n",
    "    gen = UNetGenerator().to(device)\n",
    "    disc = PatchDiscriminator().to(device)\n",
    "\n",
    "    # === æœ€é©åŒ–ã¨æå¤±é–¢æ•° ===\n",
    "    opt_g = torch.optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    opt_d = torch.optim.Adam(disc.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    bce = nn.BCELoss()\n",
    "    l1 = nn.L1Loss()\n",
    "\n",
    "    print(f\"\\n=== CGAN (pix2pix) å­¦ç¿’é–‹å§‹ [CPUãƒ¢ãƒ¼ãƒ‰ / GPUåŒæ¡ä»¶] ===\")\n",
    "    print(f\"å…¥åŠ›: {ae_dir}\")\n",
    "    print(f\"æ­£è§£: {clean_dir}\")\n",
    "    print(f\"å‡ºåŠ›: {save_root}\\n\")\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    for epoch in range(100):  # âœ… GPUç‰ˆã¨åŒã˜100ã‚¨ãƒãƒƒã‚¯\n",
    "        epoch_start = time.time()\n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch+1}/100\")\n",
    "\n",
    "        for ae_tensor, clean_tensor in loop:\n",
    "            ae_tensor, clean_tensor = ae_tensor.to(device), clean_tensor.to(device)\n",
    "\n",
    "            # --- Discriminator ---\n",
    "            fake_img = gen(ae_tensor).detach()\n",
    "            real_pred = disc(ae_tensor, clean_tensor)\n",
    "            fake_pred = disc(ae_tensor, fake_img)\n",
    "            real_loss = bce(real_pred, torch.ones_like(real_pred))\n",
    "            fake_loss = bce(fake_pred, torch.zeros_like(fake_pred))\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            opt_d.zero_grad()\n",
    "            d_loss.backward()\n",
    "            opt_d.step()\n",
    "\n",
    "            # --- Generator ---\n",
    "            fake_img = gen(ae_tensor)\n",
    "            pred = disc(ae_tensor, fake_img)\n",
    "            adv_loss = bce(pred, torch.ones_like(pred))\n",
    "            l1_loss_val = l1(fake_img, clean_tensor) * 100\n",
    "            g_loss = adv_loss + l1_loss_val\n",
    "            opt_g.zero_grad()\n",
    "            g_loss.backward()\n",
    "            opt_g.step()\n",
    "\n",
    "            loop.set_postfix(G_loss=g_loss.item(), D_loss=d_loss.item())\n",
    "\n",
    "        # --- å‡ºåŠ›ä¿å­˜ ---\n",
    "        with torch.no_grad():\n",
    "            sample = (fake_img * 0.5 + 0.5).clamp(0, 1)\n",
    "            save_image(sample, os.path.join(save_img_dir, f\"fake_epoch{epoch+1}.png\"))\n",
    "        torch.save(gen.state_dict(), os.path.join(save_weight_dir, f\"gen_weights_epoch{epoch+1}.pth\"))\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"ðŸ•’ Epoch {epoch+1} å®Œäº†: {epoch_time/60:.2f} åˆ†\")\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\n=== å­¦ç¿’å®Œäº†ï¼ˆCPU / GPUåŒæ¡ä»¶ï¼‰ ===\")\n",
    "    print(f\"â± ç·å­¦ç¿’æ™‚é–“: {total_time/60:.2f} åˆ† ({total_time/3600:.2f} æ™‚é–“)\")\n",
    "    print(f\"ç”Ÿæˆç”»åƒ: {save_img_dir}\")\n",
    "    print(f\"ãƒ¢ãƒ‡ãƒ«é‡ã¿: {save_weight_dir}\")\n",
    "    print(\"==========================================================\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === å®Ÿè¡Œ ===\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    train_cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ad58ba5-9903-4b4a-8a83-6b085a21c702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CGANæŽ¨è«–ï¼‹åˆ†é¡žé–‹å§‹ (data2/FGSM) | LEGACY_PREPROCESS=True ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:49<00:00, 20.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CGANåˆ†é¡žçµæžœCSV: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore\\data2_FGSM\\pix2pix_class_results_data2_FGSM.csv\n",
      "âœ… å¾©å…ƒç”»åƒ: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore\\data2_FGSM\n",
      "\n",
      "=== 4æ®µéšŽCSVçµ±åˆï¼†æˆåŠŸçŽ‡ (data2/FGSM) ===\n",
      "\n",
      "=== çµ±è¨ˆçµæžœ ===\n",
      "ç·æ•°: 1000\n",
      "æ”»æ’ƒæˆåŠŸæ•°:        1000/1000 (100.00%)\n",
      "WaveletçŸ¯æ­£æˆåŠŸæ•°: 810/1000 (81.00%)\n",
      "CGANçŸ¯æ­£æˆåŠŸæ•°:   945/1000 (94.50%)\n",
      "\n",
      "âœ… å‡ºåŠ›: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\compare_data2_FGSM_Wavelet_CGAN.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Waveletï¼‹CGANå¾©å…ƒï¼‹4æ®µéšŽæ¯”è¼ƒï¼†æˆåŠŸçŽ‡åˆ¤å®šï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ï¼šResize/Cropå«ã‚€ transforms() ã‚’æ—¢å®šONï¼‰\n",
    "# ==========================================================\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torchvision import models\n",
    "from torchvision.utils import save_image\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========= 0) å†ç¾æ€§ï¼ˆä»»æ„ï¼‰ =========\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ========= 1) è¨­å®šï¼šã“ã“ã ã‘å¤‰ãˆã‚‹ =========\n",
    "dataset     = \"data2\"        # \"data1\" / \"data2\" / \"data3\"\n",
    "attack_type = \"FGSM\"         # \"FGSM\" / \"CW\"\n",
    "LEGACY_PREPROCESS = True     # âœ… å‰å›žã¨åŒã˜ transforms() ã‚’ä½¿ã†ï¼ˆResize/Cropå«ã‚€ï¼‰\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\")\n",
    "\n",
    "# å…¥åŠ›/å‡ºåŠ›ãƒ‘ã‚¹ï¼ˆå‰å›žã®æ§‹æˆã«æº–æ‹ ï¼‰\n",
    "clean_dir    = BASE / dataset\n",
    "adv_dir      = BASE / f\"{dataset}_{attack_type}\"\n",
    "wavelet_dir  = adv_dir / \"wavelet_eval\"\n",
    "tensor_dir   = wavelet_dir / \"wavelet_tensor\"  # *_wavelet.pt\n",
    "\n",
    "# CGANå‡ºåŠ›ï¼ˆdataset+attackã§æ•´ç†ï¼‰\n",
    "cgan_out_dir = BASE / \"pix2pix_restore\" / f\"{dataset}_{attack_type}\"\n",
    "cgan_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CSVãƒ‘ã‚¹ï¼ˆå‰å›žã®å‘½åã«æ¥µåŠ›åˆã‚ã›ã‚‹ï¼‰\n",
    "before_csv   = clean_dir / \"clean_dataset_labels.csv\"\n",
    "after_csv    = adv_dir / f\"{attack_type}_Results.csv\"\n",
    "wavelet_csv  = wavelet_dir / f\"{attack_type}_Wavelet_class_results.csv\"\n",
    "cgan_csv     = cgan_out_dir / f\"pix2pix_class_results_{dataset}_{attack_type}.csv\"\n",
    "merged_csv   = BASE / f\"compare_{dataset}_{attack_type}_Wavelet_CGAN.csv\"\n",
    "\n",
    "# CGANé‡ã¿ï¼ˆå‰å›žãƒ‘ã‚¹ã«æº–æ‹ ï¼‰\n",
    "weight_path  = BASE / \"pix2pix_train_v2\" / \"weights\" / \"gen_weights_epoch300.pth\"\n",
    "\n",
    "# ========= 2) äº‹å‰ãƒã‚§ãƒƒã‚¯ =========\n",
    "def _must_exist(p: Path, hint: str = \"\"):\n",
    "    if not p.exists():\n",
    "        print(f\"[ERROR] Not found: {p}\\n{('  Hint: '+hint) if hint else ''}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "_must_exist(before_csv,  \"clean_dataset_labels.csv ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "_must_exist(after_csv,   f\"{attack_type}_Results.csv ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "_must_exist(wavelet_csv, f\"{attack_type}_Wavelet_class_results.csv ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "_must_exist(tensor_dir,  \"wavelet_eval/wavelet_tensor ã« *_wavelet.pt ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\")\n",
    "_must_exist(weight_path, \"pix2pix å­¦ç¿’æ¸ˆã¿é‡ã¿ã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "if not any(str(p).endswith(\".pt\") for p in tensor_dir.iterdir()):\n",
    "    print(f\"[ERROR] No .pt under: {tensor_dir}ï¼ˆ*_wavelet.pt å¿…é ˆï¼‰\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ========= 3) MobileNetV2ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›å‰å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ =========\n",
    "def load_classifier(device):\n",
    "    weights = models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "    model = models.mobilenet_v2(weights=weights).to(device).eval()\n",
    "    categories = weights.meta[\"categories\"]\n",
    "    if LEGACY_PREPROCESS:\n",
    "        # âœ… å‰å›žã¨åŒã˜ï¼šResize(256)â†’CenterCrop(224)â†’ToTensorâ†’Normalize\n",
    "        preprocess = weights.transforms()\n",
    "    else:\n",
    "        # Normalizeã®ã¿ï¼ˆè§£åƒåº¦æ®ãˆç½®ãï¼‰ã«åˆ‡ã‚Šæ›¿ãˆãŸã„å ´åˆã¯ã“ã¡ã‚‰\n",
    "        from torchvision import transforms as T\n",
    "        preprocess = T.Normalize(mean=weights.meta[\"mean\"], std=weights.meta[\"std\"])\n",
    "    return model, preprocess, categories\n",
    "\n",
    "# ========= 4) CGAN Generator =========\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            self.block(features * 4, features * 8),\n",
    "            self.block(features * 8, features * 8),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upblock(features * 8, features * 8),\n",
    "            self.upblock(features * 8 * 2, features * 4),\n",
    "            self.upblock(features * 4 * 2, features * 2),\n",
    "            self.upblock(features * 2 * 2, features),\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upblock(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x); skips.append(x)\n",
    "        skips = skips[:-1][::-1]\n",
    "        for i, layer in enumerate(self.decoder[:-2]):\n",
    "            x = layer(x)\n",
    "            if i < len(skips):\n",
    "                x = torch.cat([x, skips[i]], 1)\n",
    "        return self.decoder[-2](x)\n",
    "\n",
    "# ========= 5) CGANå¾©å…ƒï¼‹åˆ†é¡ž â†’ CSV =========\n",
    "def restore_and_classify():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gen = UNetGenerator().to(device)\n",
    "    gen.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    gen.eval()\n",
    "\n",
    "    clf, preprocess, categories = load_classifier(device)\n",
    "    results = []\n",
    "    print(f\"\\n=== CGANæŽ¨è«–ï¼‹åˆ†é¡žé–‹å§‹ ({dataset}/{attack_type}) | LEGACY_PREPROCESS={LEGACY_PREPROCESS} ===\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for fname in tqdm(sorted(os.listdir(tensor_dir))):\n",
    "            if not fname.endswith(\".pt\"):\n",
    "                continue\n",
    "            t = torch.load(tensor_dir / fname, map_location=device)  # [C,H,W] or [1,C,H,W] in [0,1]\n",
    "            if t.ndim == 3:\n",
    "                t = t.unsqueeze(0)\n",
    "            # Waveletå¾Œãƒ†ãƒ³ã‚½ãƒ«[0,1] â†’ CGANå…¥åŠ›ã¯[-1,1]\n",
    "            t_in = (t - 0.5) / 0.5 if (t.min() >= 0 and t.max() <= 1) else t\n",
    "\n",
    "            restored = gen(t_in.to(device))\n",
    "            restored_01 = (restored * 0.5 + 0.5).clamp(0, 1)  # [0,1]\n",
    "\n",
    "            # åˆ†é¡žï¼šãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ã®å ´åˆã¯ transforms() ã‚’ãã®ã¾ã¾é©ç”¨\n",
    "            # transforms() ã¯ PIL/Tensor ä¸¡å¯¾å¿œï¼†å†…éƒ¨ã§ToTensor/Normalizeç­‰ã‚’å‡¦ç†\n",
    "            x = restored_01.squeeze(0).cpu()\n",
    "            x = preprocess(x).unsqueeze(0).to(device)\n",
    "            preds = F.softmax(clf(x), dim=1)\n",
    "            conf, idx = preds.max(1)\n",
    "            label = categories[int(idx)]\n",
    "            confidence = float(conf.item())\n",
    "\n",
    "            base = Path(fname).stem.replace(\"_wavelet\", \"\")\n",
    "            out_png = cgan_out_dir / f\"{base}_{attack_type}_restored.png\"\n",
    "            save_image(restored_01.cpu(), out_png)\n",
    "\n",
    "            results.append({\n",
    "                \"ImageId\": base,\n",
    "                \"CGAN_Label\": label,\n",
    "                \"CGAN_Confidence\": round(confidence, 6),\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(results).to_csv(cgan_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… CGANåˆ†é¡žçµæžœCSV: {cgan_csv}\")\n",
    "    print(f\"âœ… å¾©å…ƒç”»åƒ: {cgan_out_dir}\")\n",
    "\n",
    "# ========= 6) 4æ®µéšŽCSVçµ±åˆï¼‹æˆåŠŸçŽ‡ =========\n",
    "def merge_and_evaluate():\n",
    "    print(f\"\\n=== 4æ®µéšŽCSVçµ±åˆï¼†æˆåŠŸçŽ‡ ({dataset}/{attack_type}) ===\\n\")\n",
    "    df_b  = pd.read_csv(before_csv)\n",
    "    df_a  = pd.read_csv(after_csv)\n",
    "    df_w  = pd.read_csv(wavelet_csv)\n",
    "    df_g  = pd.read_csv(cgan_csv)\n",
    "\n",
    "    df_b[\"ImageId\"] = df_b[\"image\"].apply(lambda x: Path(str(x)).stem)\n",
    "    df_a[\"ImageId\"] = df_a[\"image\"].apply(lambda x: Path(str(x)).stem)\n",
    "    df_g[\"ImageId\"] = df_g[\"ImageId\"].astype(str).apply(\n",
    "        lambda s: Path(s).stem.replace(f\"_{attack_type}_restored\",\"\").replace(\"_restored\",\"\").replace(\"_wavelet\",\"\")\n",
    "    )\n",
    "\n",
    "    df_b = df_b.rename(columns={\"label_name\":\"label_name_before\",\"confidence\":\"confidence_before\"})\n",
    "    df_a = df_a.rename(columns={\"Advs_label_name\":\"label_name_after\",\"Advs_confidence\":\"confidence_after\"})\n",
    "\n",
    "    df = (df_b.merge(df_a, on=\"ImageId\", how=\"inner\")\n",
    "              .merge(df_w, on=\"ImageId\", how=\"left\")\n",
    "              .merge(df_g, on=\"ImageId\", how=\"left\"))\n",
    "\n",
    "    df[\"AttackSuccess\"]            = df[\"label_name_before\"] != df[\"label_name_after\"]\n",
    "    df[\"RecoverySuccess_Wavelet\"]  = df[\"label_name_before\"] == df.get(\"Wavelet_Label\")\n",
    "    df[\"RecoverySuccess_CGAN\"]     = df[\"label_name_before\"] == df.get(\"CGAN_Label\")\n",
    "\n",
    "    df.to_csv(merged_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    total = len(df)\n",
    "    atk   = int(df[\"AttackSuccess\"].sum())\n",
    "    w_ok  = int(df[\"RecoverySuccess_Wavelet\"].sum())\n",
    "    g_ok  = int(df[\"RecoverySuccess_CGAN\"].sum())\n",
    "    pct   = lambda n: (n/total*100.0) if total else 0.0\n",
    "\n",
    "    print(\"=== çµ±è¨ˆçµæžœ ===\")\n",
    "    print(f\"ç·æ•°: {total}\")\n",
    "    print(f\"æ”»æ’ƒæˆåŠŸæ•°:        {atk}/{total} ({pct(atk):.2f}%)\")\n",
    "    print(f\"WaveletçŸ¯æ­£æˆåŠŸæ•°: {w_ok}/{total} ({pct(w_ok):.2f}%)\")\n",
    "    print(f\"CGANçŸ¯æ­£æˆåŠŸæ•°:   {g_ok}/{total} ({pct(g_ok):.2f}%)\")\n",
    "    print(f\"\\nâœ… å‡ºåŠ›: {merged_csv}\")\n",
    "\n",
    "# ========= 7) å®Ÿè¡Œ =========\n",
    "if __name__ == \"__main__\":\n",
    "    restore_and_classify()\n",
    "    merge_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ef1421-914b-4762-a48a-9ac5120c096b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CGAN (pix2pix) å­¦ç¿’é–‹å§‹ (MobileNet Feature Match) ===\n",
      "â˜… L1 Loss é‡ã¿ (LAMBDA_L1): 100\n",
      "â˜… Feature Loss é‡ã¿ (LAMBDA_FEATURE): 0\n",
      "å…¥åŠ›: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data1_FGSM\\tensor\n",
      "å‡ºåŠ›: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_train_FGSM\n",
      "\n",
      "Pre-calculating Clean Image Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:07<00:00, 17.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-calculation finished.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 119/125 [00:07<00:00, 16.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 354\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==========================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 354\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 299\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    296\u001b[0m opt_g\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# --- ãƒ­ã‚®ãƒ³ã‚°ç”¨ã®ç´¯ç© ---\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m epoch_g_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mg_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m epoch_d_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    301\u001b[0m epoch_adv_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m adv_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# CGAN (pix2pix) å­¦ç¿’: Wavelet Tensor â†’ Clean Tensor\n",
    "# â˜… MobileNetV2 Feature Matching Loss å°Žå…¥ç‰ˆ (KeyError ä¿®æ­£æ¸ˆã¿) â˜…\n",
    "# ==========================================================\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchvision import models\n",
    "from torch.nn.functional import l1_loss\n",
    "\n",
    "# ==========================================================\n",
    "# === ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š ===\n",
    "# ==========================================================\n",
    "LAMBDA_L1 = 100\n",
    "LAMBDA_FEATURE = 0\n",
    "NUM_EPOCHS = 300\n",
    "LEARNING_RATE = 2e-4    \n",
    "\n",
    "# ==========================================================\n",
    "# === Dataset (Tensorå…¥åŠ›å¯¾å¿œ) ===\n",
    "# ==========================================================\n",
    "class AE2CleanTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    AE (æ•µå¯¾çš„ã‚µãƒ³ãƒ—ãƒ«Tensor) ã¨ Clean (æ­£è§£ç”»åƒTensor) ã®ãƒšã‚¢ã‚’èª­ã¿è¾¼ã‚€Datasetã€‚\n",
    "    å€¤ãŒ [0,1] ã®å ´åˆã¯è‡ªå‹•ã§ [-1,1] ã«æ­£è¦åŒ–ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, ae_dir, clean_dir, auto_normalize=True):\n",
    "        self.ae_dir = ae_dir\n",
    "        self.clean_dir = clean_dir\n",
    "        self.auto_normalize = auto_normalize\n",
    "        self.filenames = sorted(os.listdir(ae_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ae_path = os.path.join(self.ae_dir, self.filenames[idx])\n",
    "        clean_path = os.path.join(self.clean_dir, self.filenames[idx])\n",
    "\n",
    "        ae_tensor = torch.load(ae_path)\n",
    "        clean_tensor = torch.load(clean_path)\n",
    "\n",
    "        if ae_tensor.ndim == 4:\n",
    "            ae_tensor = ae_tensor.squeeze(0)\n",
    "        if clean_tensor.ndim == 4:\n",
    "            clean_tensor = clean_tensor.squeeze(0)\n",
    "\n",
    "        if ae_tensor.shape[0] == 1:\n",
    "            ae_tensor = ae_tensor.repeat(3, 1, 1)\n",
    "        if clean_tensor.shape[0] == 1:\n",
    "            clean_tensor = clean_tensor.repeat(3, 1, 1)\n",
    "\n",
    "        if self.auto_normalize:\n",
    "            # [0, 1] -> [-1, 1] ã«æ­£è¦åŒ–\n",
    "            if ae_tensor.min() >= 0.0 and ae_tensor.max() <= 1.0:\n",
    "                ae_tensor = (ae_tensor - 0.5) / 0.5\n",
    "            if clean_tensor.min() >= 0.0 and clean_tensor.max() <= 1.0:\n",
    "                clean_tensor = (clean_tensor - 0.5) / 0.5\n",
    "\n",
    "        return ae_tensor, clean_tensor\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === Generator (U-Netæ§‹é€ ) ===\n",
    "# ==========================================================\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            self.block(features * 4, features * 8),\n",
    "            self.block(features * 8, features * 8),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upblock(features * 8, features * 8),\n",
    "            self.upblock(features * 8 * 2, features * 4),\n",
    "            self.upblock(features * 4 * 2, features * 2),\n",
    "            self.upblock(features * 2 * 2, features),\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upblock(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        skips = skips[:-1][::-1]\n",
    "\n",
    "        for idx, layer in enumerate(self.decoder[:-2]):\n",
    "            x = layer(x)\n",
    "            if idx < len(skips):\n",
    "                x = torch.cat([x, skips[idx]], 1)\n",
    "\n",
    "        return self.decoder[-2](x)  # ConvTranspose\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === Discriminator (PatchGAN) ===\n",
    "# ==========================================================\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=6, features=64): # å…¥åŠ›ã¯ Input(3ch) + Target/Fake(3ch) ã§ 6ch\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            # æœ€çµ‚å±¤\n",
    "            nn.Conv2d(features * 4, 1, kernel_size=4, stride=1, padding=1),\n",
    "            nn.Sigmoid() # 0-1ã®ç¢ºçŽ‡ã§ãƒ‘ãƒƒãƒãŒæœ¬ç‰©ã‹åˆ¤å®š\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.net(torch.cat([x, y], 1))\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === Feature Extractor (MobileNetV2) ===\n",
    "# ==========================================================\n",
    "class MobileNetFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV2ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€æŒ‡å®šã—ãŸå±¤ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚\n",
    "    åˆ†é¡žå™¨ã®ã€Œç›®ã€ã‚’Generatorã«çµ„ã¿è¾¼ã‚€ãŸã‚ã«ä½¿ç”¨ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        weights = models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "        \n",
    "        # ImageNetå­¦ç¿’æ¸ˆã¿ã®MobileNetV2ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€é‡ã¿ã‚’å›ºå®š (evalãƒ¢ãƒ¼ãƒ‰)\n",
    "        mobilenet = models.mobilenet_v2(weights=weights).to(device).eval()\n",
    "        \n",
    "        # Generatorã«ç›´æŽ¥åˆ†é¡žå™¨ã®é‡ã¿ã‚’å­¦ç¿’ã•ã›ãŸããªã„ãŸã‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å›ºå®š\n",
    "        for param in mobilenet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # ç‰¹å¾´é‡æŠ½å‡ºã«ä½¿ç”¨ã™ã‚‹å±¤ (ä¸­é–“å±¤ã¨ã—ã¦ features[:14] ã¾ã§ã‚’ä½¿ç”¨)\n",
    "        self.feature_extractor = mobilenet.features[:14] \n",
    "        \n",
    "        # â˜… ä¿®æ­£: KeyErrorã‚’é¿ã‘ã‚‹ãŸã‚ã€ImageNetæ¨™æº–ã®meanã¨stdã‚’ç›´æŽ¥è¨­å®š\n",
    "        IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "        IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        self.mean = torch.tensor(IMAGENET_MEAN).view(1, 3, 1, 1).to(device)\n",
    "        self.std = torch.tensor(IMAGENET_STD).view(1, 3, 1, 1).to(device)\n",
    "        \n",
    "    def forward(self, x_01):\n",
    "        \"\"\"\n",
    "        å…¥åŠ›: [0, 1] ã®ç¯„å›²ã®ç”»åƒãƒ†ãƒ³ã‚½ãƒ« ([B, C, H, W])\n",
    "        \"\"\"\n",
    "        # MobileNetã®æ¨™æº–çš„ãªå‰å‡¦ç†ï¼ˆæ­£è¦åŒ–ï¼‰\n",
    "        x_norm = (x_01 - self.mean) / self.std\n",
    "        \n",
    "        # ç‰¹å¾´é‡ã‚’æŠ½å‡º\n",
    "        features = self.feature_extractor(x_norm)\n",
    "        return features\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# === å­¦ç¿’ãƒ«ãƒ¼ãƒ— ===\n",
    "# ==========================================================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # === ãƒ‘ã‚¹è¨­å®š ===\n",
    "    ae_dir = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data1_FGSM\\tensor\"\n",
    "    clean_dir = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\data1_clean_tensor\"\n",
    "    \n",
    "    # â˜… ãƒ•ã‚©ãƒ«ãƒ€åã«Feature Lossã®é‡ã¿ã‚’è¿½åŠ \n",
    "    save_root = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_train_FGSM\"\n",
    "    save_img_dir = os.path.join(save_root, \"images\")\n",
    "    save_weight_dir = os.path.join(save_root, \"weights\")\n",
    "    save_log_dir = os.path.join(save_root, \"logs\")\n",
    "    os.makedirs(save_img_dir, exist_ok=True)\n",
    "    os.makedirs(save_weight_dir, exist_ok=True)\n",
    "    os.makedirs(save_log_dir, exist_ok=True)\n",
    "\n",
    "    dataset = AE2CleanTensorDataset(ae_dir, clean_dir)\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    gen = UNetGenerator().to(device)\n",
    "    disc = PatchDiscriminator().to(device)\n",
    "    feature_extractor = MobileNetFeatureExtractor(device) # â˜… MobileNetV2ã®ç‰¹å¾´é‡æŠ½å‡ºå™¨\n",
    "\n",
    "    opt_g = torch.optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    opt_d = torch.optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    bce = nn.BCELoss()\n",
    "    l1 = nn.L1Loss()\n",
    "\n",
    "    # â˜… ãƒ­ã‚°ãƒªã‚¹ãƒˆã« Feature Loss ã‚’è¿½åŠ \n",
    "    G_loss_list, D_loss_list = [], []\n",
    "    Adv_loss_list, L1_loss_list = [], []\n",
    "    Feature_loss_list = []\n",
    "\n",
    "    print(f\"\\n=== CGAN (pix2pix) å­¦ç¿’é–‹å§‹ (MobileNet Feature Match) ===\")\n",
    "    print(f\"â˜… L1 Loss é‡ã¿ (LAMBDA_L1): {LAMBDA_L1}\")\n",
    "    print(f\"â˜… Feature Loss é‡ã¿ (LAMBDA_FEATURE): {LAMBDA_FEATURE}\") # â˜… è¡¨ç¤º\n",
    "    print(f\"å…¥åŠ›: {ae_dir}\")\n",
    "    print(f\"å‡ºåŠ›: {save_root}\\n\")\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    # --- Utility: [-1, 1] -> [0, 1] ---\n",
    "    def to_01(t):\n",
    "        return (t * 0.5 + 0.5).clamp(0, 1)\n",
    "\n",
    "    # --- Clean Tensorã®ç‰¹å¾´é‡ã‚’äº‹å‰ã«è¨ˆç®— ---\n",
    "    clean_features_list = []\n",
    "    print(\"Pre-calculating Clean Image Features...\")\n",
    "    with torch.no_grad():\n",
    "        for _, clean_tensor in tqdm(loader):\n",
    "            # Clean Tensor [-1, 1] ã‚’ MobileNetV2 ã®å…¥åŠ›ç¯„å›² [0, 1] ã«æˆ»ã—ã¦ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º\n",
    "            clean_features = feature_extractor(to_01(clean_tensor.to(device)))\n",
    "            clean_features_list.append(clean_features)\n",
    "    \n",
    "    print(\"Pre-calculation finished.\\n\")\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        epoch_g_loss, epoch_d_loss = 0, 0\n",
    "        epoch_adv_loss, epoch_l1_loss = 0, 0\n",
    "        epoch_feature_loss = 0 # â˜… è¿½åŠ \n",
    "\n",
    "        # Clean Feature Iteratorã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "        clean_features_iter = iter(clean_features_list)\n",
    "\n",
    "        for ae_tensor, clean_tensor in loop:\n",
    "            ae_tensor, clean_tensor = ae_tensor.to(device), clean_tensor.to(device)\n",
    "            current_clean_features = next(clean_features_iter) # äº‹å‰è¨ˆç®—ã—ãŸç‰¹å¾´é‡ã‚’å–å¾—\n",
    "\n",
    "            # --- Discriminatorã®å­¦ç¿’ ---\n",
    "            fake_img = gen(ae_tensor).detach()\n",
    "            real_pred = disc(ae_tensor, clean_tensor)\n",
    "            fake_pred = disc(ae_tensor, fake_img)\n",
    "            real_loss = bce(real_pred, torch.ones_like(real_pred))\n",
    "            fake_loss = bce(fake_pred, torch.zeros_like(fake_pred))\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            opt_d.zero_grad()\n",
    "            d_loss.backward()\n",
    "            opt_d.step()\n",
    "\n",
    "            # --- Generatorã®å­¦ç¿’ ---\n",
    "            fake_img = gen(ae_tensor)\n",
    "            \n",
    "            # 1. Adversarial Loss\n",
    "            pred = disc(ae_tensor, fake_img)\n",
    "            adv_loss = bce(pred, torch.ones_like(pred))\n",
    "            \n",
    "            # 2. L1 Loss\n",
    "            l1_loss_val = l1(fake_img, clean_tensor) * LAMBDA_L1\n",
    "            \n",
    "            # 3. â˜… Feature Matching Loss\n",
    "            fake_img_01 = to_01(fake_img)\n",
    "            fake_features = feature_extractor(fake_img_01)\n",
    "            \n",
    "            # ç‰¹å¾´é‡ãƒžãƒƒãƒ—é–“ã®L1è·é›¢ã‚’è¨ˆç®— (Feature Matching Loss)\n",
    "            feature_loss_val = l1_loss(fake_features, current_clean_features) * LAMBDA_FEATURE\n",
    "            \n",
    "            # Total Generator Loss\n",
    "            g_loss = adv_loss + l1_loss_val + feature_loss_val\n",
    "            \n",
    "            opt_g.zero_grad()\n",
    "            g_loss.backward()\n",
    "            opt_g.step()\n",
    "\n",
    "            # --- ãƒ­ã‚®ãƒ³ã‚°ç”¨ã®ç´¯ç© ---\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            epoch_adv_loss += adv_loss.item()\n",
    "            epoch_l1_loss += l1_loss_val.item()\n",
    "            epoch_feature_loss += feature_loss_val.item()\n",
    "\n",
    "        # --- ã‚¨ãƒãƒƒã‚¯å¹³å‡ã®è¨˜éŒ² ---\n",
    "        G_loss_list.append(epoch_g_loss / len(loader))\n",
    "        D_loss_list.append(epoch_d_loss / len(loader))\n",
    "        Adv_loss_list.append(epoch_adv_loss / len(loader))\n",
    "        L1_loss_list.append(epoch_l1_loss / len(loader))\n",
    "        Feature_loss_list.append(epoch_feature_loss / len(loader))\n",
    "\n",
    "        # --- ç”»åƒ/é‡ã¿ä¿å­˜ ---\n",
    "        with torch.no_grad():\n",
    "            sample = to_01(fake_img)\n",
    "            save_image(sample, os.path.join(save_img_dir, f\"fake_epoch{epoch+1}.png\"))\n",
    "\n",
    "        torch.save(gen.state_dict(), os.path.join(save_weight_dir, f\"gen_weights_epoch{epoch+1}.pth\"))\n",
    "\n",
    "        print(f\"ðŸ•’ Epoch {epoch+1} å®Œäº† | G_Loss: {G_loss_list[-1]:.4f} (Adv:{Adv_loss_list[-1]:.4f}, L1:{L1_loss_list[-1]:.4f}, Feat:{Feature_loss_list[-1]:.4f}) | D_Loss: {D_loss_list[-1]:.4f}\")\n",
    "\n",
    "    # --- ãƒ­ã‚°/ã‚°ãƒ©ãƒ•ä¿å­˜ ---\n",
    "    df_log = pd.DataFrame({\n",
    "        \"Epoch\": range(1, NUM_EPOCHS + 1),\n",
    "        \"G_loss\": G_loss_list,\n",
    "        \"Adv_loss\": Adv_loss_list,\n",
    "        \"L1_loss\": L1_loss_list,\n",
    "        \"Feature_loss\": Feature_loss_list,\n",
    "        \"D_loss\": D_loss_list\n",
    "    })\n",
    "    df_log.to_csv(os.path.join(save_log_dir, \"loss_history.csv\"), index=False)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(G_loss_list, label=\"Generator Total Loss\", linewidth=2)\n",
    "    plt.plot(Adv_loss_list, label=\"Adv Loss\", linestyle='--', alpha=0.7)\n",
    "    plt.plot(L1_loss_list, label=\"L1 Loss (Weighted)\", linestyle='--', alpha=0.7)\n",
    "    plt.plot(Feature_loss_list, label=\"Feature Loss (Weighted)\", linestyle='--', alpha=0.7)\n",
    "    plt.plot(D_loss_list, label=\"Discriminator Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training Loss Curve (L1={LAMBDA_L1}, Feat={LAMBDA_FEATURE})\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_log_dir, \"loss_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\n=== å­¦ç¿’å®Œäº† ===\")\n",
    "    print(f\"â± ç·å­¦ç¿’æ™‚é–“: {total_time/60:.2f} åˆ†\")\n",
    "    print(f\"å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: {save_root}\")\n",
    "    print(\"==========================================================\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11b5021e-34ba-4499-b55f-a29a78c31543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\attack_compare_top100.docx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torchvision import models\n",
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "\n",
    "# === è¨­å®š ===\n",
    "csv_path = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore_FGSM_train\\data3_CW\\compare_data3_CW_Wavelet_CGAN.csv\"\n",
    "output_docx = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\attack_compare_top100.docx\"\n",
    "num_rows = 20  # å‡ºåŠ›ä»¶æ•°\n",
    "\n",
    "# === ImageNetã‚¯ãƒ©ã‚¹åãƒªã‚¹ãƒˆ ===\n",
    "weights = models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "imagenet_labels = weights.meta[\"categories\"]\n",
    "\n",
    "# === CSVèª­ã¿è¾¼ã¿ ===\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# === class_XXX â†’ ãƒ©ãƒ™ãƒ«åã«å¤‰æ›é–¢æ•° ===\n",
    "def convert_class_to_label(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    if isinstance(x, str) and x.startswith(\"class_\"):\n",
    "        try:\n",
    "            idx = int(x.replace(\"class_\", \"\").strip())\n",
    "            return imagenet_labels[idx]\n",
    "        except:\n",
    "            return x\n",
    "    return x\n",
    "\n",
    "# å¯¾è±¡åˆ—ã‚’å¤‰æ›ï¼ˆæ”»æ’ƒå¾Œãƒ»CGANãƒ»Waveletãªã©ï¼‰\n",
    "for col in [\"label_name_after\", \"Wavelet_Label\", \"CGAN_Label\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(convert_class_to_label)\n",
    "\n",
    "# === å¿…è¦åˆ—ã ã‘æŠ½å‡º ===\n",
    "cols = [\n",
    "    \"label_name_before\", \"confidence_before\",\n",
    "    \"label_name_after\", \"confidence_after\",\n",
    "    \"CGAN_Label\", \"CGAN_Confidence\"\n",
    "]\n",
    "df_show = df[cols].head(num_rows).copy()\n",
    "\n",
    "# === confidence ã‚’å°æ•°6æ¡ã«ä¸¸ã‚ã‚‹ ===\n",
    "for col in [\"confidence_before\", \"confidence_after\", \"CGAN_Confidence\"]:\n",
    "    df_show[col] = df_show[col].round(6)\n",
    "\n",
    "# === åˆ—åã‚’æ—¥æœ¬èªžã«å¤‰æ›´ ===\n",
    "japanese_columns = {\n",
    "    \"label_name_before\": \"å…¥åŠ›ç”»åƒãƒ©ãƒ™ãƒ«\",\n",
    "    \"confidence_before\": \"confidenceï¼ˆå…¥åŠ›ï¼‰\",\n",
    "    \"label_name_after\": \"æ”»æ’ƒå¾Œãƒ©ãƒ™ãƒ«\",\n",
    "    \"confidence_after\": \"confidenceï¼ˆæ”»æ’ƒå¾Œï¼‰\",\n",
    "    \"CGAN_Label\": \"CGANå¾©å…ƒå¾Œãƒ©ãƒ™ãƒ«\",\n",
    "    \"CGAN_Confidence\": \"confidenceï¼ˆå¾©å…ƒå¾Œï¼‰\"\n",
    "}\n",
    "df_show.rename(columns=japanese_columns, inplace=True)\n",
    "\n",
    "# === Word å‡ºåŠ› ===\n",
    "doc = Document()\n",
    "doc.add_heading(\"æ•µå¯¾çš„ã‚µãƒ³ãƒ—ãƒ«æ¯”è¼ƒè¡¨ï¼ˆä¸Šä½20ä»¶ï¼‰\", level=1)\n",
    "doc.add_paragraph(\"é€šå¸¸ç”»åƒ â†’ æ”»æ’ƒå¾Œç”»åƒ â†’ Wavelet+CGANå¾©å…ƒå¾Œ\")\n",
    "\n",
    "table = doc.add_table(rows=1, cols=len(df_show.columns))\n",
    "table.style = \"Table Grid\"\n",
    "\n",
    "# ãƒ˜ãƒƒãƒ€è¡Œ\n",
    "hdr_cells = table.rows[0].cells\n",
    "for i, col in enumerate(df_show.columns):\n",
    "    hdr_cells[i].text = col\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿è¡Œ\n",
    "for _, row in df_show.iterrows():\n",
    "    row_cells = table.add_row().cells\n",
    "    for i, col in enumerate(df_show.columns):\n",
    "        row_cells[i].text = str(row[col])\n",
    "\n",
    "# === ä¿å­˜ ===\n",
    "doc.save(output_docx)\n",
    "print(f\"âœ… Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {output_docx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881bf1a3-1aad-46d4-9f3a-4967cfadc908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CGANæŽ¨è«–ï¼‹åˆ†é¡žé–‹å§‹ (data3/CW) ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:45<00:00, 22.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CGANåˆ†é¡žçµæžœCSV: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore_FGSM_train\\data3_CW\\pix2pix_class_results_data3_CW.csv\n",
      "âœ… å¾©å…ƒç”»åƒ: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore_FGSM_train\\data3_CW\n",
      "\n",
      "=== 4æ®µéšŽCSVçµ±åˆï¼†æˆåŠŸçŽ‡ (data3/CW) ===\n",
      "\n",
      "=== çµ±è¨ˆçµæžœ ===\n",
      "ç·æ•°: 1000\n",
      "æ”»æ’ƒæˆåŠŸæ•°:        1000/1000 (100.00%)\n",
      "WaveletçŸ¯æ­£æˆåŠŸæ•°: 917/1000 (91.70%)\n",
      "CGANçŸ¯æ­£æˆåŠŸæ•°:   942/1000 (94.20%)\n",
      "\n",
      "âœ… å‡ºåŠ›: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore_FGSM_train\\data3_CW\\compare_data3_CW_Wavelet_CGAN.csv\n",
      "âŒ WaveletçŸ¯æ­£å¤±æ•—ç”»åƒã‚’ä¿å­˜: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\wavelet_restore_failed\\data3_CW\n",
      "âŒ CGANçŸ¯æ­£å¤±æ•—ç”»åƒã‚’ä¿å­˜: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\cgan_restore_failed\\data3_CW\n",
      "ðŸ“ å¤±æ•—ä¸€è¦§CSV: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore_FGSM_train\\data3_CW\\failed_lists_data3_CW.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Waveletï¼‹CGANå¾©å…ƒï¼‹4æ®µéšŽæ¯”è¼ƒï¼†æˆåŠŸçŽ‡åˆ¤å®šï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ï¼šResize/Cropå«ã‚€ transforms() ã‚’æ—¢å®šONï¼‰\n",
    "# + WaveletçŸ¯æ­£å¤±æ•— / CGANçŸ¯æ­£å¤±æ•— ç”»åƒã®è‡ªå‹•ä»•åˆ†ã‘ä¿å­˜\n",
    "# + ã™ã¹ã¦ã®CSVï¼ˆCGANåˆ†é¡žçµæžœãƒ»çµ±åˆçµæžœãƒ»å¤±æ•—ä¸€è¦§ï¼‰ã¯ã€Œç”»åƒã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆcgan_out_dirï¼‰ã€ã«å‡ºåŠ›\n",
    "# ==========================================================\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torchvision import models\n",
    "from torchvision.utils import save_image\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from shutil import copy2\n",
    "\n",
    "# ========= 0) å†ç¾æ€§ï¼ˆä»»æ„ï¼‰ =========\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ========= 1) è¨­å®šï¼šã“ã“ã ã‘å¤‰ãˆã‚‹ =========\n",
    "dataset     = \"data3\"        # \"data1\" / \"data2\" / \"data3\"\n",
    "attack_type = \"CW\"         # \"FGSM\" / \"CW\"\n",
    "LEGACY_PREPROCESS = True     # âœ… transforms() ã‚’ä½¿ã†ï¼ˆResize/Cropå«ã‚€å‰å›žäº’æ›ï¼‰\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\")\n",
    "\n",
    "# ========= å…¥å‡ºåŠ›ãƒ‘ã‚¹ =========\n",
    "clean_dir    = BASE / dataset\n",
    "adv_dir      = BASE / f\"{dataset}_{attack_type}\"\n",
    "wavelet_dir  = adv_dir / \"wavelet_eval\"\n",
    "tensor_dir   = wavelet_dir / \"wavelet_tensor\"\n",
    "wavelet_png_dir = wavelet_dir / \"wavelet_png\"\n",
    "\n",
    "# å¾©å…ƒç”»åƒã®å‡ºåŠ›å…ˆï¼ˆï¼æœ¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ã€Œä¸»ãƒ•ã‚©ãƒ«ãƒ€ã€ï¼‰\n",
    "cgan_out_dir = BASE / \"pix2pix_restore_train_v2\" / f\"{dataset}_{attack_type}\"\n",
    "cgan_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CSVã¯å…¨éƒ¨ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆï¼ç”»åƒã¨åŒã˜ï¼‰ã¸\n",
    "cgan_csv   = cgan_out_dir / f\"pix2pix_class_results_{dataset}_{attack_type}.csv\"\n",
    "merged_csv = cgan_out_dir / f\"compare_{dataset}_{attack_type}_Wavelet_CGAN.csv\"\n",
    "\n",
    "# å…¥åŠ›CSVï¼ˆå‚ç…§ã®ã¿ï¼‰\n",
    "before_csv   = clean_dir / \"clean_dataset_labels.csv\"\n",
    "after_csv    = adv_dir / f\"{attack_type}_Results.csv\"\n",
    "wavelet_csv  = wavelet_dir / f\"{attack_type}_Wavelet_class_results.csv\"\n",
    "\n",
    "# âœ… å¤±æ•—ç”»åƒã®å‡ºåŠ›å…ˆï¼ˆç”»åƒã¯å¾“æ¥ã©ãŠã‚Šå„ãƒ•ã‚©ãƒ«ãƒ€ã¸ä»•åˆ†ã‘ï¼‰\n",
    "wavelet_failed_dir = BASE / \"wavelet_restore_failed\" / f\"{dataset}_{attack_type}\"\n",
    "cgan_failed_dir    = BASE / \"cgan_restore_failed\"    / f\"{dataset}_{attack_type}\"\n",
    "wavelet_failed_dir.mkdir(parents=True, exist_ok=True)\n",
    "cgan_failed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# å¤±æ•—ä¸€è¦§CSVã‚‚ç”»åƒã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆcgan_out_dirï¼‰ã¸\n",
    "# â†’ å‚ç…§ãƒ»å…±æœ‰ã®ã—ã‚„ã™ã•ã‚’å„ªå…ˆ\n",
    "failed_list_csv = cgan_out_dir / f\"failed_lists_{dataset}_{attack_type}.csv\"\n",
    "\n",
    "# å­¦ç¿’æ¸ˆã¿Generatorã®é‡ã¿\n",
    "weight_path  = BASE / \"pix2pix_train_v2\" / \"weights\" / \"gen_weights_epoch300.pth\"\n",
    "\n",
    "# ========= 2) äº‹å‰ãƒã‚§ãƒƒã‚¯ =========\n",
    "def _must_exist(p: Path, hint: str = \"\"):\n",
    "    if not p.exists():\n",
    "        print(f\"[ERROR] Not found: {p}\")\n",
    "        if hint:\n",
    "            print(\"Hint:\", hint)\n",
    "        sys.exit(1)\n",
    "\n",
    "_must_exist(before_csv,  \"clean_dataset_labels.csv ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "_must_exist(after_csv,   f\"{attack_type}_Results.csv ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "_must_exist(wavelet_csv, f\"{attack_type}_Wavelet_class_results.csv ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "_must_exist(tensor_dir,  \"wavelet_eval/wavelet_tensor ã« *_wavelet.pt ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\")\n",
    "_must_exist(weight_path, \"pix2pix å­¦ç¿’æ¸ˆã¿é‡ã¿ã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "if not any(str(p).endswith(\".pt\") for p in tensor_dir.iterdir()):\n",
    "    print(f\"[ERROR] No .pt under: {tensor_dir}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ========= 3) MobileNetV2ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›å‰å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ =========\n",
    "def load_classifier(device):\n",
    "    weights = models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "    model = models.mobilenet_v2(weights=weights).to(device).eval()\n",
    "    categories = weights.meta[\"categories\"]\n",
    "    if LEGACY_PREPROCESS:\n",
    "        preprocess = weights.transforms()\n",
    "    else:\n",
    "        from torchvision import transforms as T\n",
    "        preprocess = T.Normalize(mean=weights.meta[\"mean\"], std=weights.meta[\"std\"])\n",
    "    return model, preprocess, categories\n",
    "\n",
    "# ========= 4) CGAN Generator =========\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            self.block(features * 4, features * 8),\n",
    "            self.block(features * 8, features * 8),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upblock(features * 8, features * 8),\n",
    "            self.upblock(features * 8 * 2, features * 4),\n",
    "            self.upblock(features * 4 * 2, features * 2),\n",
    "            self.upblock(features * 2 * 2, features),\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upblock(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x); skips.append(x)\n",
    "        skips = skips[:-1][::-1]\n",
    "        for i, layer in enumerate(self.decoder[:-2]):\n",
    "            x = layer(x)\n",
    "            if i < len(skips):\n",
    "                x = torch.cat([x, skips[i]], 1)\n",
    "        return self.decoder[-2](x)\n",
    "\n",
    "# ========= 5) CGANå¾©å…ƒï¼‹åˆ†é¡ž â†’ CSVï¼ˆç”»åƒã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ï¼‰ =========\n",
    "def restore_and_classify():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gen = UNetGenerator().to(device)\n",
    "    gen.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    gen.eval()\n",
    "\n",
    "    clf, preprocess, categories = load_classifier(device)\n",
    "    results = []\n",
    "\n",
    "    print(f\"\\n=== CGANæŽ¨è«–ï¼‹åˆ†é¡žé–‹å§‹ ({dataset}/{attack_type}) ===\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for fname in tqdm(sorted(os.listdir(tensor_dir))):\n",
    "            if not fname.endswith(\".pt\"):\n",
    "                continue\n",
    "            t = torch.load(tensor_dir / fname, map_location=device)\n",
    "            if t.ndim == 3:\n",
    "                t = t.unsqueeze(0)\n",
    "\n",
    "            # [0,1] ãªã‚‰ [-1,1] ã«æ­£è¦åŒ–ã—ã¦Generatorã¸\n",
    "            t_in = (t - 0.5) / 0.5 if (t.min() >= 0 and t.max() <= 1) else t\n",
    "            restored = gen(t_in.to(device))\n",
    "            restored_01 = (restored * 0.5 + 0.5).clamp(0, 1)\n",
    "\n",
    "            # åˆ†é¡žï¼ˆå¿…è¦ãªã‚‰ãƒ¬ã‚¬ã‚·ãƒ¼ã®Resize/Cropå«ã‚€ transforms()ï¼‰\n",
    "            x = restored_01.squeeze(0).cpu()\n",
    "            x = preprocess(x).unsqueeze(0).to(device)\n",
    "            preds = F.softmax(clf(x), dim=1)\n",
    "            conf, idx = preds.max(1)\n",
    "            label = categories[int(idx)]\n",
    "            confidence = float(conf.item())\n",
    "\n",
    "            # ç”»åƒä¿å­˜\n",
    "            base = Path(fname).stem.replace(\"_wavelet\", \"\")\n",
    "            out_png = cgan_out_dir / f\"{base}_{attack_type}_restored.png\"\n",
    "            save_image(restored_01.cpu(), out_png)\n",
    "\n",
    "            # çµæžœè¨˜éŒ²\n",
    "            results.append({\n",
    "                \"ImageId\": base,\n",
    "                \"CGAN_Label\": label,\n",
    "                \"CGAN_Confidence\": round(confidence, 6),\n",
    "            })\n",
    "\n",
    "    # âœ… CSVã¯ç”»åƒã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«å‡ºåŠ›\n",
    "    pd.DataFrame(results).to_csv(cgan_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… CGANåˆ†é¡žçµæžœCSV: {cgan_csv}\")\n",
    "    print(f\"âœ… å¾©å…ƒç”»åƒ: {cgan_out_dir}\")\n",
    "\n",
    "# ========= 6) 4æ®µéšŽCSVçµ±åˆï¼‹æˆåŠŸçŽ‡ + å¤±æ•—ç”»åƒä¿å­˜ï¼‹å¤±æ•—ä¸€è¦§CSVï¼ˆã™ã¹ã¦ç”»åƒã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ï¼‰ =========\n",
    "def merge_and_evaluate():\n",
    "    print(f\"\\n=== 4æ®µéšŽCSVçµ±åˆï¼†æˆåŠŸçŽ‡ ({dataset}/{attack_type}) ===\\n\")\n",
    "    df_b  = pd.read_csv(before_csv)\n",
    "    df_a  = pd.read_csv(after_csv)\n",
    "    df_w  = pd.read_csv(wavelet_csv)\n",
    "    df_g  = pd.read_csv(cgan_csv)\n",
    "\n",
    "    df_b[\"ImageId\"] = df_b[\"image\"].apply(lambda x: Path(str(x)).stem)\n",
    "    df_a[\"ImageId\"] = df_a[\"image\"].apply(lambda x: Path(str(x)).stem)\n",
    "    df_g[\"ImageId\"] = df_g[\"ImageId\"].astype(str).apply(\n",
    "        lambda s: Path(s).stem.replace(f\"_{attack_type}_restored\",\"\").replace(\"_restored\",\"\").replace(\"_wavelet\",\"\")\n",
    "    )\n",
    "\n",
    "    df_b = df_b.rename(columns={\"label_name\":\"label_name_before\",\"confidence\":\"confidence_before\"})\n",
    "    df_a = df_a.rename(columns={\"Advs_label_name\":\"label_name_after\",\"Advs_confidence\":\"confidence_after\"})\n",
    "\n",
    "    df = (df_b.merge(df_a, on=\"ImageId\", how=\"inner\")\n",
    "              .merge(df_w, on=\"ImageId\", how=\"left\")\n",
    "              .merge(df_g, on=\"ImageId\", how=\"left\"))\n",
    "\n",
    "    df[\"AttackSuccess\"]            = df[\"label_name_before\"] != df[\"label_name_after\"]\n",
    "    df[\"RecoverySuccess_Wavelet\"]  = df[\"label_name_before\"] == df.get(\"Wavelet_Label\")\n",
    "    df[\"RecoverySuccess_CGAN\"]     = df[\"label_name_before\"] == df.get(\"CGAN_Label\")\n",
    "\n",
    "    # âœ… çµ±åˆçµæžœCSVã‚‚ç”»åƒã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã¸\n",
    "    df.to_csv(merged_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    total = len(df)\n",
    "    atk   = int(df[\"AttackSuccess\"].sum())\n",
    "    w_ok  = int(df[\"RecoverySuccess_Wavelet\"].sum())\n",
    "    g_ok  = int(df[\"RecoverySuccess_CGAN\"].sum())\n",
    "    pct   = lambda n: (n/total*100.0) if total else 0.0\n",
    "\n",
    "    print(\"=== çµ±è¨ˆçµæžœ ===\")\n",
    "    print(f\"ç·æ•°: {total}\")\n",
    "    print(f\"æ”»æ’ƒæˆåŠŸæ•°:        {atk}/{total} ({pct(atk):.2f}%)\")\n",
    "    print(f\"WaveletçŸ¯æ­£æˆåŠŸæ•°: {w_ok}/{total} ({pct(w_ok):.2f}%)\")\n",
    "    print(f\"CGANçŸ¯æ­£æˆåŠŸæ•°:   {g_ok}/{total} ({pct(g_ok):.2f}%)\")\n",
    "    print(f\"\\nâœ… å‡ºåŠ›: {merged_csv}\")\n",
    "\n",
    "    # ---- WaveletçŸ¯æ­£å¤±æ•—ç”»åƒã®ä¿å­˜ ----\n",
    "    wavelet_failed = df[df[\"RecoverySuccess_Wavelet\"] == False]\n",
    "    if len(wavelet_failed) > 0:\n",
    "        for _, row in wavelet_failed.iterrows():\n",
    "            img_id = str(row[\"ImageId\"])\n",
    "            src = wavelet_png_dir / f\"{img_id}_wavelet.png\"\n",
    "            if src.exists():\n",
    "                dst = wavelet_failed_dir / src.name\n",
    "                copy2(src, dst)\n",
    "        print(f\"âŒ WaveletçŸ¯æ­£å¤±æ•—ç”»åƒã‚’ä¿å­˜: {wavelet_failed_dir}\")\n",
    "    else:\n",
    "        print(\"âœ… WaveletçŸ¯æ­£ã¯å…¨ä»¶æˆåŠŸ\")\n",
    "\n",
    "    # ---- CGANçŸ¯æ­£å¤±æ•—ç”»åƒã®ä¿å­˜ ----\n",
    "    cgan_failed = df[df[\"RecoverySuccess_CGAN\"] == False]\n",
    "    if len(cgan_failed) > 0:\n",
    "        for _, row in cgan_failed.iterrows():\n",
    "            img_id = str(row[\"ImageId\"])\n",
    "            cgan_img_path = cgan_out_dir / f\"{img_id}_{attack_type}_restored.png\"\n",
    "            if cgan_img_path.exists():\n",
    "                dst = cgan_failed_dir / cgan_img_path.name\n",
    "                copy2(cgan_img_path, dst)\n",
    "        print(f\"âŒ CGANçŸ¯æ­£å¤±æ•—ç”»åƒã‚’ä¿å­˜: {cgan_failed_dir}\")\n",
    "    else:\n",
    "        print(\"âœ… CGANçŸ¯æ­£ã¯å…¨ä»¶æˆåŠŸ\")\n",
    "\n",
    "    # âœ… å¤±æ•—ä¸€è¦§CSVã¯ç”»åƒã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆcgan_out_dirï¼‰ã¸\n",
    "    out_cols = [\n",
    "        \"ImageId\",\n",
    "        \"label_name_before\",\n",
    "        \"label_name_after\",\n",
    "        \"Wavelet_Label\",\n",
    "        \"CGAN_Label\",\n",
    "        \"confidence_before\",\n",
    "        \"confidence_after\",\n",
    "        \"CGAN_Confidence\"\n",
    "    ]\n",
    "    failed_concat = pd.concat(\n",
    "        [\n",
    "            wavelet_failed.assign(FailStage=\"Wavelet\"),\n",
    "            cgan_failed.assign(FailStage=\"CGAN\")\n",
    "        ],\n",
    "        axis=0, ignore_index=True\n",
    "    )\n",
    "    if len(failed_concat) > 0:\n",
    "        failed_concat[out_cols + [\"FailStage\"]].to_csv(failed_list_csv, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"ðŸ“ å¤±æ•—ä¸€è¦§CSV: {failed_list_csv}\")\n",
    "    else:\n",
    "        print(\"ðŸ“ å¤±æ•—ä¸€è¦§CSV: å¤±æ•—ãªã—ã®ãŸã‚å‡ºåŠ›ã‚¹ã‚­ãƒƒãƒ—\")\n",
    "\n",
    "# ========= 7) å®Ÿè¡Œ =========\n",
    "if __name__ == \"__main__\":\n",
    "    restore_and_classify()\n",
    "    merge_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "072bbe19-22be-4c63-8fc2-5743b0e4584b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ã‚¯ãƒ©ã‚¹ID â†’ ãƒ©ãƒ™ãƒ«åå¤‰æ›å®Œäº†ã€‚ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torchvision import models\n",
    "\n",
    "# === ImageNetã‚¯ãƒ©ã‚¹åãƒªã‚¹ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ ===\n",
    "weights = models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "imagenet_labels = weights.meta[\"categories\"]\n",
    "\n",
    "# === CSVèª­ã¿è¾¼ã¿ ===\n",
    "csv_path = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore_FGSM_train\\data2_CW\\compare_data2_CW_Wavelet_CGAN.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# === class_XXX â†’ ãƒ©ãƒ™ãƒ«åã«å¤‰æ› ===\n",
    "def convert_class_to_label(x):\n",
    "    if isinstance(x, str) and x.startswith(\"class_\"):\n",
    "        try:\n",
    "            idx = int(x.replace(\"class_\", \"\"))\n",
    "            return imagenet_labels[idx]\n",
    "        except:\n",
    "            return x\n",
    "    return x\n",
    "\n",
    "df[\"label_name_after\"] = df[\"label_name_after\"].apply(convert_class_to_label)\n",
    "\n",
    "# Wavelet / CGAN åˆ—ã‚‚å¤‰æ›ãŒå¿…è¦ãªå ´åˆã¯è¿½åŠ \n",
    "for col in [\"Wavelet_Label\", \"CGAN_Label\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(convert_class_to_label)\n",
    "\n",
    "# === ä¿å­˜ï¼ˆä¸Šæ›¸ã or åˆ¥åä¿å­˜ï¼‰ ===\n",
    "df.to_csv(csv_path.replace(\".csv\", \"_fixed.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(\"âœ… ã‚¯ãƒ©ã‚¹ID â†’ ãƒ©ãƒ™ãƒ«åå¤‰æ›å®Œäº†ã€‚ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46dd9848-7cb3-4a33-9124-85a8b674b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ã‚¯ãƒ©ã‚¹ID â†’ ImageNetãƒ©ãƒ™ãƒ«åã«å¤‰æ›å®Œäº†\n",
      "ä¿å­˜å…ˆ: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore_FGSM_train\\data2_CW\\compare_data2_CW_Wavelet_CGAN_label_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torchvision import models\n",
    "\n",
    "# === ImageNetã‚¯ãƒ©ã‚¹åãƒªã‚¹ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ ===\n",
    "weights = models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "imagenet_labels = weights.meta[\"categories\"]\n",
    "\n",
    "# === CSVèª­ã¿è¾¼ã¿ ===\n",
    "csv_path = r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore_FGSM_train\\data2_CW\\compare_data2_CW_Wavelet_CGAN.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# === class_XXX â†’ ImageNetãƒ©ãƒ™ãƒ«åã«å¤‰æ› ===\n",
    "def convert_class_to_label(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    if isinstance(x, (int, float)):  # ã™ã§ã«æ•°å­—ï¼ˆã‚¯ãƒ©ã‚¹IDï¼‰ãªã‚‰ãã®ã¾ã¾å¯¾å¿œ\n",
    "        idx = int(x)\n",
    "        if 0 <= idx < len(imagenet_labels):\n",
    "            return imagenet_labels[idx]\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s.startswith(\"class_\"):\n",
    "            try:\n",
    "                idx = int(s.replace(\"class_\", \"\").strip())\n",
    "                return imagenet_labels[idx]\n",
    "            except:\n",
    "                return s\n",
    "        elif s.isdigit():\n",
    "            try:\n",
    "                idx = int(s)\n",
    "                return imagenet_labels[idx]\n",
    "            except:\n",
    "                return s\n",
    "        return s\n",
    "    return x\n",
    "\n",
    "# === å¯¾è±¡åˆ—ã‚’ä¸€æ‹¬å¤‰æ› ===\n",
    "target_cols = [\n",
    "    \"Original_label_name\",\n",
    "    \"Advs_label_name\",\n",
    "    \"Wavelet_Label\",\n",
    "    \"CGAN_Label\"\n",
    "]\n",
    "\n",
    "for col in target_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(convert_class_to_label)\n",
    "\n",
    "# === ä¿å­˜ï¼ˆåˆ¥åä¿å­˜ã—ã¦ä¸Šæ›¸ãé˜²æ­¢ï¼‰ ===\n",
    "out_path = csv_path.replace(\".csv\", \"_label_fixed.csv\")\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… ã‚¯ãƒ©ã‚¹ID â†’ ImageNetãƒ©ãƒ™ãƒ«åã«å¤‰æ›å®Œäº†\\nä¿å­˜å…ˆ: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262f789d-9277-47fb-bd03-4a8b61036f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Waveletï¼‹CGAN Tensorç”Ÿæˆé–‹å§‹ (data3/FGSM) ===\n",
      "\n",
      "å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore_train_v2\\data3_FGSM\\tensor_out\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:06<00:00, 143.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… æ–°è¦ç”Ÿæˆ: 1000 ä»¶\n",
      "â© ã‚¹ã‚­ãƒƒãƒ—: 0 ä»¶ï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰\n",
      "ðŸ“ å‡ºåŠ›å…ˆ: C:\\Users\\sit\\wavelet_CGAN\\testdata3\\pix2pix_restore_train_v2\\data3_FGSM\\tensor_out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Waveletï¼‹CGAN å¾©å…ƒTensorã®ã¿å‡ºåŠ›ï¼ˆæœ€å°ãƒ»å›ºå®šãƒ•ã‚©ãƒ«ãƒ€ç‰ˆï¼‰\n",
    "# å‡ºåŠ›å½¢å¼: sample0001.pt, sample0002.pt, ...\n",
    "# å‡ºåŠ›å…ˆ: pix2pix_restore_train_v2/<dataset>_<attack_type>/tensor_out/\n",
    "# æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä¸Šæ›¸ãé˜²æ­¢ï¼‰\n",
    "# ==========================================================\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= 1) è¨­å®š =========\n",
    "dataset     = \"data3\"        # \"data1\" / \"data2\" / \"data3\"\n",
    "attack_type = \"FGSM\"           # \"FGSM\" / \"CW\"\n",
    "BASE        = Path(r\"C:\\Users\\sit\\wavelet_CGAN\\testdata3\")\n",
    "\n",
    "tensor_dir  = BASE / f\"{dataset}_{attack_type}\" / \"wavelet_eval\" / \"wavelet_tensor\"\n",
    "weight_path = BASE / \"pix2pix_train_v2\" / \"weights\" / \"gen_weights_epoch300.pth\"\n",
    "\n",
    "# å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆå›ºå®šï¼‰\n",
    "tensor_out_dir = BASE / \"pix2pix_restore_train_v2\" / f\"{dataset}_{attack_type}\" / \"tensor_out\"\n",
    "tensor_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= 2) ãƒã‚§ãƒƒã‚¯ =========\n",
    "def _must_exist(p: Path, msg: str):\n",
    "    if not p.exists():\n",
    "        print(f\"[ERROR] {msg}: {p}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "_must_exist(tensor_dir, \"wavelet_tensor ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“\")\n",
    "_must_exist(weight_path, \"Generator é‡ã¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# ========= 3) UNet Generatorå®šç¾© =========\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.block(in_channels, features, normalize=False),\n",
    "            self.block(features, features * 2),\n",
    "            self.block(features * 2, features * 4),\n",
    "            self.block(features * 4, features * 8),\n",
    "            self.block(features * 8, features * 8),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upblock(features * 8, features * 8),\n",
    "            self.upblock(features * 8 * 2, features * 4),\n",
    "            self.upblock(features * 4 * 2, features * 2),\n",
    "            self.upblock(features * 2 * 2, features),\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def block(self, in_c, out_c, normalize=True):\n",
    "        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upblock(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "        skips = skips[:-1][::-1]\n",
    "        for i, layer in enumerate(self.decoder[:-2]):\n",
    "            x = layer(x)\n",
    "            if i < len(skips):\n",
    "                x = torch.cat([x, skips[i]], 1)\n",
    "        return self.decoder[-2](x)\n",
    "\n",
    "# ========= 4) å¾©å…ƒTensorç”Ÿæˆ =========\n",
    "def restore_tensor_only():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gen = UNetGenerator().to(device)\n",
    "    gen.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    gen.eval()\n",
    "\n",
    "    print(f\"\\n=== Waveletï¼‹CGAN Tensorç”Ÿæˆé–‹å§‹ ({dataset}/{attack_type}) ===\\n\")\n",
    "    print(f\"å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: {tensor_out_dir}\\n\")\n",
    "\n",
    "    count = 0\n",
    "    skipped = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for fname in tqdm(sorted(os.listdir(tensor_dir))):\n",
    "            if not fname.endswith(\".pt\"):\n",
    "                continue\n",
    "\n",
    "            out_path = tensor_out_dir / Path(fname).name\n",
    "            if out_path.exists():\n",
    "                skipped += 1\n",
    "                continue  # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "\n",
    "            # Wavelet tensor èª­ã¿è¾¼ã¿\n",
    "            t = torch.load(tensor_dir / fname, map_location=device)\n",
    "            if t.ndim == 3:\n",
    "                t = t.unsqueeze(0)\n",
    "            t_in = (t - 0.5) / 0.5 if (t.min() >= 0 and t.max() <= 1) else t\n",
    "\n",
    "            # Generatorã§å¾©å…ƒ\n",
    "            restored = gen(t_in.to(device))\n",
    "            restored_01 = (restored * 0.5 + 0.5).clamp(0, 1)\n",
    "\n",
    "            # sample0001.pt ã®å½¢å¼ã§ä¿å­˜\n",
    "            torch.save(restored_01.cpu(), out_path)\n",
    "            count += 1\n",
    "\n",
    "    print(f\"\\nâœ… æ–°è¦ç”Ÿæˆ: {count} ä»¶\")\n",
    "    print(f\"â© ã‚¹ã‚­ãƒƒãƒ—: {skipped} ä»¶ï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰\")\n",
    "    print(f\"ðŸ“ å‡ºåŠ›å…ˆ: {tensor_out_dir}\")\n",
    "\n",
    "# ========= 5) å®Ÿè¡Œ =========\n",
    "if __name__ == \"__main__\":\n",
    "    restore_tensor_only()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c45ee-e6c7-4c06-bf0a-dd2aab66d872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch260)",
   "language": "python",
   "name": "torch260"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
